# o-brian-et-al-2020-clinical-trials-of-adult-stuttering-treatment-comparison-of-percentage-syllables-stuttered-with-self

```
JSLHR

Research Note

Clinical Trials of Adult Stuttering Treatment:
Comparison of Percentage Syllables
Stuttered With Self-Reported Stuttering
Severity as Primary Outcomes

Sue O’Brian,a Rob Heard,b Mark Onslow,a

Ann Packman,a

Robyn Lowe,a

and Ross G. Menziesa

Purpose: In a companion paper, we found no statistical
reason to favor percentage syllables stuttered (%SS)
over parent-reported stuttering severity as a primary
outcome measure for clinical trials of early stuttering.
Hence, considering the logistical advantages of the latter
measure, we recommended parent-reported stuttering
severity for use as an outcome measure. The present
report extends the prior analysis to a comparison of
%SS with self-reported stuttering severity (SRSS) for
use as an outcome measure in clinical trials of stuttering
treatments for adults.
Method: We analyzed data from four randomized clinical
trials for adults that incorporated %SS and SRSS data
at prerandomization and at 6 months post randomization.
We analyzed the distributions associated with the
two measures, their agreement, and their estimates of
effect sizes.

Results: The positively skewed distribution of %SS warrants
much reservation about its value as a clinical trial outcome
measure. This skew causes inherent instability because
of spurious data associated with low scores, which occur
commonly at the low end of such a distribution. This inherent
instability is compounded by inherent problems with absolute
reliability of %SS measures. These problems are reduced
with the much more normal distribution of SRSS.
Conclusions: The logistical arguments in favor of SRSS
apply similarly to adults as they do when parents report
the stuttering severity of their children. However, there are
statistical reasons to favor SRSS over %SS measures as
a primary outcome of clinical trials with adult participants:
SRSS has acceptable discriminant validity and a normal
distribution, and it is less error prone than %SS. We
recommend SRSS as a primary outcome for clinical trials
of adults with stuttering.

I n a companion paper (Onslow et al., 2018), we ar-

gued that randomized clinical trials are generally ac-
cepted as the gold standard for evaluating health care

efficacy, which includes stuttering treatment research. In
that paper, we noted that virtually all clinical trials of stut-
tering treatment for adults at that time used percentage syl-
lables stuttered (%SS) as an outcome measure. We noted
the advantages of this measure to be its transparent con-
nection with the purpose of a trial focused on stuttering re-
duction. Additionally, it appears that %SS scores during

aAustralian Stuttering Research Centre, The University of Technology
Sydney, Australia
bFaculty of Health Sciences, The University of Sydney, Australia
Correspondence to Mark Onslow: Mark.Onslow@uts.edu.au

Editor-in-Chief: Bharath Chandrasekaran
Editor: Julie D. Anderson
Received August 19, 2019
Revision received October 8, 2019
Accepted January 22, 2020
https://doi.org/10.1044/2020_JSLHR-19-00142

a 10-min unscheduled telephone call provide a valid repre-
sentation of %SS scores during an entire day of speaking
(Karimi et al., 2013).

In Onslow et al. (2018), we noted that, compared

with other cognate health science disciplines, there has been
much research about the disorder of stuttering but com-
paratively few randomized clinical trials. At present, only
three randomized trials have been published about the effi-
cacy of methods to control stuttering with adults (Carey
et al., 2010; Cream et al., 2010; Menzies, O’Brian, et al.,
2019). Onslow et al. (2018) argued that a factor causing
this slow clinical development might be logistical and
economic barriers associated with using the %SS measure.
These logistical barriers involve obtaining representative
prerandomization and postrandomization audio or (prefer-
ably) video recordings of trial participants, having blinded
observers measure %SS, and demonstrating the absolute
and relative reliability of these data. We argued, therefore,

Disclosure: The authors have declared that no competing interests existed at the time
of publication.

Journal of Speech, Language, and Hearing Research (cid:129) Vol. 63 (cid:129) 1387–1394 (cid:129) May 2020 (cid:129) Copyright © 2020 American Speech-Language-Hearing Association 1387

Downloaded from: https://pubs.asha.org Michigan State University on 11/11/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions that a logistically simpler measure of stuttering severity
for clinical trials of early stuttering intervention—parent-
reported severity ratings—would expedite publication of
such trials.

The Onslow et al. (2018) report used data from three
randomized controlled trials of early stuttering intervention
with the Lidcombe Program (Onslow et al., 2019), which
involved children who were treated using an in-clinic, in-
dividualized format. The three trials included %SS and
parent-reported severity ratings as outcomes, and outcomes
using these two measures were compared using a number
of methods. Correlations between the two measures were
moderate. Based on median change with 95% confidence
intervals and analysis of covariance, both measures showed
large effect sizes from pretreatment to follow-up. Bland–
Altman plots showed no evidence of systematic bias for
either measure. Consequently, Onslow et al. (2018) con-
cluded that there was no statistical reason to favor %SS
over such a simple measure of outcome.

The Onslow et al. (2018) report dealt with parent-

reported severity ratings as a replacement for %SS scores
during clinical trials of early stuttering treatment. The pres-
ent report deals with an equivalent, simple score for clini-
cal trials of adult speech treatment: self-reported stuttering
severity (SRSS). There are other potential alternatives to
%SS as an outcome measure for this age group, but all
have some limitations for the present purposes of measure-
ment simplification. The Stuttering Severity Instrument
(SSI-4; Riley, 2009) is a composite measure, but it involves
recording, transcribing, and analyzing participant speech.
Additionally, it may provide no additional information than
provided by a simple severity rating scale (Lewis, 1995).
There has been an encouraging report that an automated
algorithm-based speech efficiency score may be a viable
alternative to %SS or severity rating measures (Amir et al.,
2018), but further research is needed to justify confidence
in that method. Measures dealing with speech satisfaction
have been developed (Huinck & Rietveld, 2007; Karimi
et al., 2018). Although speech satisfaction may incorporate
the dimension of stuttering severity, it does not focus spe-
cifically on that speech dimension.

The advantages of such a simple scaling procedure—
not time consuming, but taking account stuttering sever-
ity, stuttering frequency, and subjective feelings about
stuttering—were cited by Huinck and Rietveld (2007). They
designed a study of the relationship between pretreatment
to posttreatment changes in a perceptual measure of speech
satisfaction and a range of objective and subjective mea-
sures. They evaluated the validity of this scale where par-
ticipants were asked to assign a 10-point “general speech
satisfaction score,” where 1 = very bad, 2 = bad, 3 = very
strongly insufficient, 4 = strongly insufficient, 5 = insufficient,
6 = sufficient, 7 = more than sufficient, 8 = good, 9 = very
good, and 10 = excellent (p. 93). For two stuttering treat-
ments, significant correlations were reported for pretreat-
ment to immediate posttreatment change. Changes in the
general speech satisfaction score correlated significantly
with changes in %SS, speech rate, two subscales of the

perceptions of stuttering inventory (Woolf, 1967), and the
fluent-halting subscale of a comprehensive perceptual scale
(Franken et al., 1995). No correlations occurred between
changes during the period from pretreatment to 24-month
posttreatment interval.

There is evidence that clinician-judged stuttering

severity for adults using a 9-point scale is interchangeable
with %SS, with a strong correlation of .91 between the
two (O’Brian, Packman, Onslow, & O’Brian, 2004). Nota-
ble sources of discrepancy in this study were cases where
speech samples had many stuttering moments involving
repeated movements or few stuttering moments involv-
ing fixed postures: The former will increase %SS values,
whereas the latter will decrease them, and those varia-
tions will not necessarily be reflected in clinician judg-
ment of severity.

For the case of adult SRSS, apart from some early
reports (Aron, 1967; Naylor, 1953), there have been two
suggestions that such a measure would be interchangeable
with %SS. O’Brian, Packman, and Onslow (2004) studied
10 adults seeking treatment for stuttering, who used a
9-point severity scale to record SRSS during six 10-min
everyday speaking situations. Results showed that, across
the participants and the speaking situations, SRSS mostly
reflected severity and %SS. Six weeks later, the participants
listened to the recordings of their speech on which they
based their severity scores and assigned another SRSS score.
Although percentage agreement is not a comprehensive
assessment, intrajudge agreement was found to be strong,
with 75% of their second scores agreeing with their first by
within one scale value. Interjudge agreement between speech-
language pathologists for the samples was 77% agreement
within one scale value, and between speakers and speech-
language pathologists, it was 78%.

In a more comprehensive study, Karimi et al. (2014)

studied 87 treatment-seeking adults, who received an un-
scheduled 10-min telephone call. At the conclusion of the
call, the participants reported SRSS with a 9-point scale.
The conversations were recorded and measured for %SS by
three speech-language pathologists. For the three judges,
correlations of .90, .74, and .75 were reported, with the first
judge being the most experienced of the three.

Clearly, when compared with %SS, having partici-
pants provide an SRSS measure during clinical trials is
effortless and without cost. Additionally, SRSS has face
validity and can be used repeatedly in different sampling
situations without any additional researcher effort or cost.
In contrast, for each additional %SS measure, researchers
need to record a beyond-clinic conversation with a re-
search assistant and have an independent observer measure
%SS from those recordings. In order to complete these
tasks, instrumentation is required—either a button-press
device or a smartphone application—to measure %SS in
real time. Considerable training is required to achieve
reliable use of that method (La Trobe University, 2014;
University of California, 2014). However, with SRSS, all
that is required is to explain to participants how to use a
simple 9-point scale.

1388 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 63 (cid:129) 1387–1394 (cid:129) May 2020

Downloaded from: https://pubs.asha.org Michigan State University on 11/11/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions The present report extends the O’Brian, Packman,
and Onslow (2004) and Karimi et al. (2014) reports and
partially replicates the Huinck and Rietveld (2007) report
to explore the relative merits of %SS and SRSS as a pri-
mary outcome in clinical trials.

Method
Data Sets

Data for analyses were available from four random-

ized clinical trials for adults that incorporated %SS and
SRSS (N = 186). None of the studies involved a large group
effect size associated with a no-treatment control group.
The trials involved treatment variations rather than no-
treatment control groups; hence, they involved small effect
sizes. The four trials had commonalities that enabled analy-
ses of %SS and SRSS data before treatment and at 6 months
post randomization. For 153 participants, data were avail-
able at both measurement occasions.

The Cream et al. (2010) trial (N = 89) determined

the additive benefits of adding a self-modeling procedure
to speech restructuring treatment for stuttering control
with adults and adolescents. The data for the 64 adults in
this trial were used for the present report. The design was
an open-plan, parallel-group randomized controlled trial.
The control group received 5 days of intensive group-based
speech restructuring, followed by seven weekly clinic visits
for clinical maintenance with individual and small-group
formats. Subsequent to the fifth day of intensive speech
restructuring, the experimental group watched a 90-s self-
modeling video of themselves each day for 4 weeks, in ad-
dition to the maintenance procedures. The self-modeling
video contained a head and shoulders exemplar of stutter-
free, natural sounding speech. At 1 month and 6 months
postrandomization, the self-modeling procedure did not
improve %SS scores recorded during unscheduled tele-
phone calls to participants, but it showed modest improve-
ments of SRSS and quality-of-life scores.

The Carey et al. (2010) trial (N = 40) determined

the relative efficacy of telephone-based telepractice and in-
clinic speech restructuring treatment for adults. The data
from all participants in this trial were used in the present
report. The design was a parallel-group, noninferiority ran-
domized controlled trial. All participants received an indi-
vidualized version of the Camperdown Program (O’Brian
et al., 2018). The control group received treatment using
a standard face-to-face weekly visit clinical format, and the
experimental group received their treatment entirely by
telephone. Based on %SS scores obtained from unscheduled
telephone calls to participants at 9-month randomization,
outcomes for the telehealth group were not inferior to the
face-to-face group. Additionally, the telehealth group re-
quired significantly less contact treatment time (by 221 min
on average) than the control group.

The Menzies, O’Brian, et al. (2019) trial (N = 32)
determined the benefits of adding Internet-based cognitive
behavior therapy (CBT) for anxiety, known as iGlebe

(Helgadóttir et al., 2014), to a speech restructuring treat-
ment for adults. The data from all participants in this trial
were used in the present report. The design was a two-arm
randomized experimental trial, with assessments at 6 and
12 months post randomization. All participants received
3-day intensive group-based speech restructuring treatment
that was based on the Camperdown Program (O’Brian
et al., 2018) but was adapted to contain no anxiolytic com-
ponents. All participants received a 1-hr group follow-up
maintenance session each month for 5 months. After the
third intensive speech restructuring day, participants in
the experimental program received online access to the
iGlebe Internet program for 5 months. At 12 months post
randomization, the experimental group showed clinically
significant improvements to SRSS and quality-of-life scores.

The Menzies, Packman, et al. (2019) trial (N = 50)

explored the relative benefits of in-clinic and iGlebe CBT
for anxiety connected with stuttering in adults. The data
from all participants in this trial were used in the present
report. The design was a noninferiority randomized con-
trolled trial. One group received a standard, in-clinic CBT
treatment for stuttering-related anxiety (Menzies et al.,
2008), and the other group received 5 months of online
access to iGlebe. Assessments at 6 and 12 months post
randomization showed, for both groups, a medium effect
size for psychological and quality-of-life measures, with
little difference between the treatment arms. There was
evidence of improvements in the groups of SRSS scores but
not for %SS scores.

In all four trials, measurement procedures were simi-
lar for each of the two measures. %SS was measured from
two unscheduled 10-min phone calls from people unknown
to the participants. One telephone call was constrained to
“small talk,” and the other was “challenging,” comprising
participant opinions about controversial topics and inter-
ruptions from the research assistant making the call. For
the purposes of the present analyses, %SS scores from the
two phone calls were pooled for each participant to estab-
lish a “typical” %SS score.

All participants in the four trials scored their “typical”

SRSS using a scale where 1 = no stuttering and 9 = ex-
tremely severe stuttering (O’Brian et al., 2003). Participants
in the Menzies, Packman, et al. (2019) trial scored their
“typical” SRSS for the week prior to the assessment. Par-
ticipants in the Cream et al. (2010) and Menzies, O’Brian,
et al. (2019) trials scored their typical SRSS for eight indi-
vidual representative speaking situations, with the lead ques-
tion, “Write a number between 1 and 9 (where 1 = no
stuttering and 9 = extremely severe stuttering) that repre-
sents your typical stuttering severity in that situation.” The
mean of the eight scores was used in the analysis. Partici-
pants in the Carey et al. (2010) trial followed the same pro-
cedure, except only five situations were scored and the lead
question asked about average rather than typical severity.1

1The term “typical” is used henceforth to include “average” SRSS
scores.

O’Brian et al.: Comparison of Stuttering Measurements 1389

Downloaded from: https://pubs.asha.org Michigan State University on 11/11/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Data Analyses

Three methods were used to compare the %SS and
SRSS scores from prerandomization to 6 months post
randomization. First, the distribution of the two measures
at the two assessments was compared graphically. Then,
Spearman’s rho correlation was established for change in
%SS and typical SRSS from the baseline to 6-month post-
randomization. Finally, the effect sizes for percentage
change from prerandomization to 6 months post randomi-
zation were determined with a bootstrapping method
(Wright et al., 2011).

Results
Distributions

Figure 1 presents the distributions of %SS scores

and typical SRSS scores, along with means and standard
deviations, at prerandomization and at 6 months post
randomization. Consistent with existing reports (Jones
et al., 2006; O’Brian, Packman, Onslow, & O’Brian, 2004;

Soderberg, 1962), the distribution of %SS was extremely
positively skewed, and that of SRSS was more normal, with
slight positive skew.

Agreement Between Measures of Stuttering
Severity Change

The change in stuttering from the baseline to 6 months

measured by %SS was compared to the change measured
by typical SRSS. The Spearman correlation was .27 (95%
CI [0.12, 0.41]). This was statistically significant ( p < .001)
but low, with only 7.3% of the variance in one data set
explainable by the other. Figure 2 presents a Bland–Altman
z score plot of percentage change for %SS and typical
SRSS. The plot shows extreme divergence, again indicating
that the measures are not corresponding well.

Table 1 presents an analysis of the extent to which
the two measures (n = 153) showed improvement or no
improvement (elevated or equal scores) at 6 months post
randomization compared with prerandomization. For
29.4% of participants, there was disagreement between
the two scores, 95% CI [22.6, 37].

Figure 1. Distributions of percentage syllables stuttered (%SS) scores and typical self-reported stuttering severity (SRSS) scores at prerandomization
and at 6 months post randomization.

1390 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 63 (cid:129) 1387–1394 (cid:129) May 2020

Downloaded from: https://pubs.asha.org Michigan State University on 11/11/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Figure 2. Bland–Altman z score plot of percentage change for
percentage syllables stuttered (%SS) and typical self-reported
stuttering severity (SRSS).

for SRSS. The nonsignificance of the effect size for %SS is
related to the volatility of the measure, as described above.

Discussion

We found statistical evidence of poor correspondence
between %SS and typical SRSS measures using correlation
and Bland–Altman analyses. Additionally, for almost a
third of participants in the clinical trials, the two measures
gave conflicting information about whether stuttering had
improved or not. This inconsonance could be the effects
of error or could be due to measures that reflect constructs
that are different in some respects, or a combination of both.
On balance, we would argue that a salient part of this
problem is that the two measures involve different constructs
to some extent; if the two constructs were similar, similar
distributions would be expected. Indeed, %SS is based on
observer counts of stuttering moments during recorded
speech, and SRSS takes account of the nature of individual
stuttering moments. O’Brian, Packman, Onslow, and O’Brian
(2004) reported that this is the source of some perturba-
tion to the correlation between %SS and SRSS. Samples
containing high proportions of repeated movements can
be reflected in high %SS scores that are not reflected in
SRSS scores because they do not necessarily impair speech
flow. In contrast, speech with high proportions of fixed
postures has the potential to cause more impairment to
speech flow than repeated movements and may be associ-
ated with higher SRSS scores.

In this context, it is of interest that Huinck and

Rietveld (2007) reported a significant correlation of .72 for
changes in their speech satisfaction score and %SS for the
pretreatment to 24-month posttreatment interval. This
was much higher than our correlation result for change in
SRSS and %SS from the baseline to 6 months, which, al-
though significant, was much lower at .27. Naturally, as
with any unidimensional subjective measure, SRSS is likely
to be influenced by many of the well-known negative emo-
tions associated with stuttering, such as anxiety (for an
overview, see Onslow, 2019) and a range of other stuttering-
related perceptions. The likelihood of such an effect is made
clear by Huinck and Rietveld’s further finding of signifi-
cant associations between changes measured with the gen-
eral speech satisfaction scale and changes measured by both
the “fluent-halting” scale and the struggle subscale of the
perceptions of stuttering inventory.

Vickers (2001) notes that, from the perspective of

statistical inference, change of scores from prerandomiza-
tion to postrandomization is not necessarily an ideal basis
for analyses of clinical trial data, yet change scores pro-
vide a transparent and accessible index of clinical outcome
from an intervention. However, the skewed %SS distribu-
tion can pose problems because change scores will com-
monly be based on small denominator values for baseline
severity, which can cause volatile change scores. These can
cause outlying data points. For example, the most extreme
percentage change score for %SS in the present cohort was

O’Brian et al.: Comparison of Stuttering Measurements 1391

Effect Sizes

Because of the differing distributions for %SS and

typical SRSS, the bootstrapping method (Wright et al., 2011)
was chosen to compare effect sizes for percentage change
from prerandomization to 6 months post randomization.
Bootstrapping is a computer-intensive method of estimating
the population mean and 95% confidence intervals of pop-
ulations from which samples are taken. Using the sample
available, many randomly resampled (“bootstrapped”) data
sets are used to provide the population estimates. For the
present analyses, data were resampled 2,000 times. Table 2
presents results separately for the control and experimental
comparison groups for the four trials. The first column
of this table shows mean effect sizes for %SS and SRSS,
measured with percentage change from prerandomization
to 6 months post randomization. The means of the change
measures from prerandomization to 6 months post randomi-
zation show quite similar effect sizes. However, for the
control comparison group for the four trials, the standard
deviation of percentage change for %SS is much larger than
for typical SRSS. For this reason, the 95% confidence in-
terval for the former contains zero, but this is not the case

Table 1. The extent to which the two measures (n = 153) showed
improvement or no improvement (elevated or equal scores) at
6 months post randomization compared with prerandomization.

Typical SRSS
change

Improvement
No improvement

%SS change

Improvement

No improvement

99
26

19
9

Note. SRSS = self-reported stuttering severity; %SS = percentage
syllables stuttered.

Downloaded from: https://pubs.asha.org Michigan State University on 11/11/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Table 2. Bootstrapping comparison of %SS and typical SRSS effect sizes from prerandomization to 6 months
post randomization. The first column shows mean effect sizes for %SS and SRSS, measured with percentage
change from prerandomization to 6 months post randomization.

Group

Control

Treatment

M (SD)

Lower 95% CI

Upper 95% CI

Minimum

Maximum

%SS
SRSS
%SS
SRSS

18.24 (79.03)
20.08 (30.73)
26.75 (53.88)
26.79 (37.52)

−2.50
12.87
10.51
15.39

35.14
27.45
42.58
37.32

−99.32
−100
−98.26
−100

420.00
68.42
116.67
84.62

Note. %SS = percentage syllables stuttered; SRSS = self-reported stuttering severity.

from 0.3 prerandomization (baseline) to 1.3 at 6 months
post randomization, giving a percentage change score of
+433%. Hence, it seems that the positively skewed distribu-
tion of %SS warrants much reservation about its value as
a clinical trial outcome measure. This skew causes inherent
instability because of spurious data associated with low
scores, which occur commonly at the low end of such a dis-
tribution. The more normal distribution of SRSS minimizes
the chance of this happening because, compared with the
%SS distribution, fewer participants will have low baseline
scores. Additionally, the 9-point structure of SRSS pro-
vides more inherent stability because it is associated with
less error variance.

This issue with the distribution of %SS measures is

compounded by their inherent absolute reliability problems.
The problem of low baseline scores occurring with a posi-
tively skewed distribution would be worsened by the well-
known reliability problems with observer counts of stuttering
moments on which %SS is based. The seminal demonstra-
tion of this reliability problem (Kully & Boberg, 1988) re-
ported stutter counts for nine 1-min speech samples made
by authorities in eight international research centers. The
minimum stutter count difference across clinics for any one
speech sample was four, and the maximum was 15. Another
demonstration of this problem (Brundage et al., 2006) as-
sociated clinical experience with double the number of stut-
ter counts for the same speech samples. This means that
errors can cause extreme change scores at the low end of
the distribution. For example, consider a 10-min speech
sample of everyday speech that, according to Karimi et al.
2013, had a mean of 1,355 syllables spoken for 10 partici-
pants. The difference between a count of one and two stut-
tering moments during such a sample, which clearly could
be error variance, doubles a %SS score from 0.07 to 0.15.
For such samples, an error of 200% would be the minimum
that could be caused by error variance. For such a sample, a
discrepancy of 10 stutter counts, which according to Kully
and Boberg (1988) and Brundage et al. (2006) is likely to
occur, would result in %SS errors greater than 1,000%. Per-
haps, paradoxically, the sparse 9-point structure of the SRSS
measure is less volatile to such meaningless changes because
the maximum possible measurement error is 100%, regard-
less of the speech sample duration on which it is based.

Much of the volatility of %SS as a measure seems

attributable to judgments about the presence of stuttering

moments in speech samples. As such, it is arguable that
the often-cited advantage of %SS as an objective measure
is illusory. The continuity hypothesis (Bloodstein, 1970)
is that stuttering and normal disfluency lie on a perceptual
continuum. While the theoretical importance of this pros-
pect seems to have diminished in recent years, arguably, it
is still salient as a perceptual feature of the disorder. If the
disorder of stuttering and normal disfluency are either ends
of a perceptual continuum, the same may apply to the
speech events that characterize the two. This would mean
that deciding whether an individual speech event is normal
disfluency or a stuttering moment would be a subjective
judgment, not an objective one, and hence, the same would
apply to the %SS measure derived from these judgments.
There is strong support for this prospect from data showing
that observers depart radically in their judgments of the
number of stuttering moments in speech samples (Brundage
et al., 2006; Kully & Boberg, 1988).

Quantitatively, %SS and SRSS generated similar

effect sizes for the four clinical trials studied. However, there
was evidence that instability of the %SS measure could
cause nonsignificant statistical comparisons from prerando-
mization to postrandomization, but the equivalent compari-
sons for SRSS would be significant. This could be interpreted
to mean that the statistical analysis of %SS change scores
in clinical trials could result in less statistical power than
the use of SRSS change scores. In short, a program of clin-
ical trial research with %SS as a primary outcome would
involve more Type II statistical errors than a program of
clinical trial research using SRSS as a primary outcome. As
noted by Vickers (2001), for some data sets, change scores
are not an optimal method for statistical analysis of clinical
trial data, and analysis of covariance of postrandomization
scores is preferable, with prerandomization scores as a covar-
iate. Even so, the problem of %SS as an unstable, error-
prone measure is not obviated. Regardless of the analysis
method, error variance will inevitably reduce the power of
a clinical trial to find effects.

Conclusions

In a companion paper (Onslow et al., 2018), we

noted the logistical simplicity of parent-reported severity
ratings for children and could find no statistical reason to
distinguish them from %SS. Hence, we recommended the

1392 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 63 (cid:129) 1387–1394 (cid:129) May 2020

Downloaded from: https://pubs.asha.org Michigan State University on 11/11/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions use of parent-reported severity ratings as a primary out-
come in clinical trials of early stuttering intervention. For
the present report, we form a stronger view. The logistical
arguments in favor of SRSS apply to adults as they do to
parents of children. Participants in clinical trials can col-
lect as many of these measures that researchers consider
necessary, with no cost, brief training, and no equipment.
However, there are statistical reasons to favor SRSS over
%SS measures as a primary outcome of clinical trials with
adult participants. The most important of those reasons is
that, compared with SRSS, %SS scores introduce error var-
iance that can reduce the statistical power of clinical trials.

On balance, we conclude that SRSS is a justifiable

measure of stuttering severity with intuitive appeal because of
its normal distribution and acceptable discriminant validity.
It has statistical appeal because the measure itself, as well
as its distribution, is less error prone than %SS. There is rea-
son to believe that the two measures deal with constructs that
are different in some respects. However, they both obviously
focus on stuttering severity, as do many clinical trials with
adults who stutter. Considering those factors and the logisti-
cal simplicity of SRSS, we recommend its use as a primary
outcome in clinical trials of adult stuttering treatment. Nat-
urally, though, this recommendation does not exclude the
use of %SS as a secondary outcome in such clinical trials.
Future research could establish a relation between
these results and the results of the Onslow et al. (2018) re-
port. The present findings pertain to adults with persistent
stuttering who feature in clinical trials focusing on stutter-
ing control. It is likely that they pertain also to adolescents,
but clearly, at some point earlier in life, during childhood,
it would become logistically and statistically more justifi-
able to replace parent-reported stuttering severity with self-
reported versions. Foreseeably, establishing insight into
that issue would require a program of statistical and quali-
tative investigation.

Acknowledgments

This research was supported by Program Grants 1132370,
402763, and 633007 from the National Health and Medical Re-
search Council of Australia.

References
Amir, O., Shapira, Y., Mick, L., & Yaruss, J. S. (2018). The
Speech Efficiency Score (SES): A time-domain measure of
speech fluency. Journal of Fluency Disorders, 58, 61–69. https://
doi.org/10.1016/j.jfludis.2018.08.001

Aron, M. L. (1967). The relationships between measurements of

stuttering behaviour. Journal of the South African Logopedic
Society, 14(1), 15–34.

Bloodstein, O. (1970). Stuttering and normal nonfluency: A conti-
nuity hypothesis. British Journal of Disorders of Communica-
tion, 5(1), 30–39. https://doi.org/10.3109/13682827009011498
Brundage, S. B., Bothe, A. K., Lengeling, A. N., & Evans, J. J. (2006).
Comparing judgments of stuttering made by students, clinicians,
and highly experienced judges. Journal of Fluency Disorders,
31(4), 271–283. https://doi.org/10.1016/j.jfludis.2006.07.002

Carey, B., O’Brian, S., Onslow, M., Block, S., Jones, M., &

Packman, A. (2010). Randomized controlled non-inferiority
trial of a telehealth treatment for chronic stuttering: The
Camperdown Program. International Journal of Language &
Communication Disorders, 45, 108–120. https://doi.org/10.3109/
13682820902763944

Cream, A., O’Brian, S., Jones, M., Block, S., Harrison, E., Lincoln,
M., Hewat, S., Packman, A., Menzies, R., & Onslow, M. (2010).
Randomized controlled trial of video self-modeling following
speech restructuring treatment for stuttering. Journal of Speech,
Language, and Hearing Research, 53(4), 887–897. https://doi.
org/10.1044/1092-4388(2009/09-0080)

Franken, M. C., Boves, L., Peters, H. F. M., & Webster, R. L.

(1995). Perceptual rating instrument for speech evaluation of
stuttering treatment. Journal of Speech and Hearing Research,
38(2), 280–288. https://doi.org/10.1044/jshr.3802.280

Helgadóttir, F. D., Menzies, R. G., Onslow, M., Packman, A., &
O’Brian, S. (2014). A standalone internet cognitive behavior
therapy treatment for social anxiety in adults who stutter:
CBTPsych. Journal of Fluency Disorders, 41, 47–54. https://
doi.org/10.1016/j.jfludis.2014.04.001

Huinck, W., & Rietveld, T. (2007). The validity of a simple out-

come measure to assess stuttering therapy. Folia Phoniatrica et
Logopaedica, 59(2), 91–99. https://doi.org/10.1159/000098342
Jones, M., Onslow, M., Packman, A., & Gebski, V. (2006). Guide-
lines for statistical analysis of percentage of syllables stuttered
data. Journal of Speech, Language, and Hearing Research,
49(4), 867–878. https://doi.org/10.1044/1092-4388(2006/062)
Karimi, H., Jones, M., O’Brian, S., & Onslow, M. (2014). Clini-
cian percent syllables stuttered, clinician severity ratings and
speaker severity ratings: Are they interchangeable. International
Journal of Language & Communication Disorders, 49(3),
364–368. https://doi.org/10.1111/1460-6984.12069

Karimi, H., O’Brian, S., Onslow, M., Jones, M., Menzies, R., &
Packman, A. (2013). Unscheduled telephone calls to measure
percent syllables stuttered during clinical trials. Journal of
Speech, Language, and Hearing Research, 56(5), 1455–1461.
https://doi.org/10.1044/1092-4388(2013/12-0264)

Karimi, H., Onslow, M., Jones, M., O’Brian, S., Packman, A.,

Menzies, R., Reilly, S., Sommer, M., & Jelčić-Jakšić, S. (2018).
The Satisfaction with Communication in Everyday Speaking
Situations (SCESS) scale: An overarching outcome measure
of treatment effect. Journal of Fluency Disorders, 58, 77–85.
https://doi.org/10.1016/j.jfludis.2018.10.002

Kully, D., & Boberg, E. (1988). An investigation of interclinic

agreement in the identification of fluent and stuttered syllables.
Journal of Fluency Disorders, 13(5), 309–318. https://doi.org/
10.1016/0094-730X(88)90001-0

La Trobe University. (2014). Stuttering counts. http://tlweb.latrobe.

edu.au/health/stutteringcounts/login.php

Lewis, K. E. (1995). Do SSI-3 scores adequately reflect observations
of stuttering behaviors. American Journal of Speech-Language
Pathology, 4(4), 46–59. https://doi.org/10.1044/1058-0360.
0404.46

Menzies, R. G., O’Brian, S., Onslow, M., Packman, A., St Clare, T.,
& Block, S. (2008). An experimental clinical trial of a cognitive-
behavior therapy package for chronic stuttering. Journal of
Speech, Language, and Hearing Research, 51(6), 1451–1464.
https://doi.org/10.1044/1092-4388(2008/07-0070)

Menzies, R., O’Brian, S., Packman, A., Jones, M., Helgadóttir,
F. D., & Onslow, M. (2019). Supplementing stuttering treat-
ment with online cognitive behavior therapy: An experimental
trial. Journal of Communication Disorders, 80, 81–91. https://
doi.org/10.1016/j.jcomdis.2019.04.003

O’Brian et al.: Comparison of Stuttering Measurements 1393

Downloaded from: https://pubs.asha.org Michigan State University on 11/11/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Menzies, R., Packman, A., Onslow, M., O’Brian, S., Jones, M., &
Helgadóttir, F. J. (2019). In-clinic and standalone Internet CBT
treatment for social anxiety in stuttering: A randomized trial
of iGlebe. Journal of Speech, Language, and Hearing Research,
62, 1614–1624. https://doi.org/10.1016/j.jfludis.2014.04.001
Naylor, R. V. (1953). A comparative study of methods of estimating
the severity of stuttering. Journal of Speech and Hearing Disor-
ders, 18(1), 30–37. https://doi.org/10.1044/jshd.1801.30

O’Brian, S., Carey, B., Onslow, M., Packman, A., & Cream, A.
(2018). The camperdown program stuttering treatment guide.
Australian Stuttering Research Centre. https://www.uts.edu.
au/research-and-teaching/our-research/australian-stuttering-
researchcentre/asrc-resources/resources

O’Brian, S., Onslow, M., Cream, A., & Packman, A. (2003). The
Camperdown Program: Outcomes of a new prolonged-speech
treatment model. Journal of Speech, Language, and Hearing
Research, 46(4), 933–946. https://doi.org/10.1044/1092-4388
(2003/073)

O’Brian, S., Packman, A., & Onslow, M. (2004). Self-rating of

stuttering severity as a clinical tool. American Journal of Speech-
Language Pathology, 13(3), 219–226. https://doi.org/10.1044/
1058-0360(2004/023)

O’Brian, S., Packman, A., Onslow, M., & O’Brian, N. (2004).

Measurement of stuttering in adults: Comparison of stuttering-
rate and severity-scaling methods. Journal of Speech, Language,
and Hearing Research, 47(5), 1081–1087. https://doi.org/10.1044/
1092-4388(2004/080)

Onslow, M. (2019). Stuttering and its treatment: Eleven lectures.
https://www.uts.edu.au/research-and-teaching/our-research/
australian-stuttering-researchcentre/asrc-resources/resources

Onslow, M., Jones, M., O’Brian, S., Packman, A., Menzies, R.,

Lowe, R., Arnott, S., Bridgman, K., de Sonneville, C., & Franken,
M.-C. (2018). Comparison of percentage syllables stuttered
with parent-reported severity ratings as a primary outcome
measure in clinical trials of early stuttering treatment. Journal
of Speech, Language, and Hearing Research, 61(4), 811–819.
https://doi.org/10.1044/2017_JSLHR-S-16-0448

Onslow, M., Webber, M., Harrison, E., Arnott, S., Bridgman, K.,
Carey, B., & Lloyd, W. (2019). The Lidcombe Program treat-
ment guide. https://www.uts.edu.au/research-andteaching/our-
research/australian-stuttering-research-centre/asrc-resources/
resources

Riley, G. (2009). Stuttering Severity Instrument–Fourth Edition:

Examiners’ Manual (SSI-4). Pro-Ed.

Soderberg, G. A. (1962). What is ‘average’ stuttering. Journal of

Speech and Hearing Disorders, 27, 85–86. https://doi.org/10.1044/
jshd.2701.85

University of California. (2014). SMS Stuttering Measurement

System. http://sms.id.ucsb.edu/index.html

Vickers, A. J. (2001). The use of percentage change from baseline
as an outcome in a controlled trial is statistically inefficient:
A simulation study. BMC Medical Research Methodology,
1(1), 6. https://doi.org/10.1186/1471-2288-1-6

Woolf, G. (1967). The assessment of stuttering as struggle, avoidance,
and expectancy. British Journal of Disorders of Communication,
2(2), 158–171. https://doi.org/10.3109/13682826709031315
Wright, D. B., Kamala, L., & Field, A. P. (2011). Using bootstrap
estimation and the plug-in principle for clinical psychology
data. Journal of Experimental Psychopathology, 2, 252–270.
https://doi.org/10.5127/jep.013611

1394 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 63 (cid:129) 1387–1394 (cid:129) May 2020

Downloaded from: https://pubs.asha.org Michigan State University on 11/11/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions
```