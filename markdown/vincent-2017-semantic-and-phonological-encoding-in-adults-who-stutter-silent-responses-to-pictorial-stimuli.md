# vincent-2017-semantic-and-phonological-encoding-in-adults-who-stutter-silent-responses-to-pictorial-stimuli

```
JSLHR

Research Article

Semantic and Phonological Encoding
in Adults Who Stutter: Silent
Responses to Pictorial Stimuli

Irena Vincenta

Purpose: Research on language planning in adult stuttering
is relatively sparse and offers diverging arguments about
a potential causative relationship between semantic and
phonological encoding and fluency breakdowns. This study
further investigated semantic and phonological encoding
efficiency in adults who stutter (AWS) by means of silent
category and phoneme identification, respectively.
Method: Fifteen AWS and 15 age- and sex-matched adults
who do not stutter (ANS) participated. The groups were
compared on the basis of the accuracy and speed of
superordinate category (animal vs. object) and initial
phoneme (vowel vs. consonant) decisions, which were
indicated manually during silent viewing of pictorial stimuli.
Movement execution latency was accounted for, and
no other cognitive, linguistic, or motor demands were
posed on participants’ responses. Therefore, category
identification accuracy and speed were considered indirect

measures of semantic encoding efficiency and phoneme
identification accuracy and speed of phonological encoding
efficiency.
Results: For category decisions, AWS were slower but not
less accurate than ANS, with objects eliciting more errors
and slower responses than animals in both groups. For
phoneme decisions, the groups did not differ in accuracy,
with consonant errors outnumbering vowel errors in both
groups, and AWS were slower than ANS in consonant but
not vowel identification, with consonant response time
lagging behind vowel response time in AWS only.
Conclusions: AWS were less efficient than ANS in
semantic encoding, and they might harbor a consonant-
specific phonological encoding weakness. Future
independent studies are warranted to discover if these
positive findings are replicable and a marker for persistent
stuttering.

D espite extensive endeavors to identify the under-

lying mechanisms of persistent childhood onset
stuttering, the etiological aspect of the disorder,

which affects approximately 1% of the population (Bloodstein
& Bernstein Ratner, 2008), remains a source of puzzlement.
Under the assumption that stuttering is a disorder of speech
execution, countless research studies have focused on the
respiratory, phonatory, and articulatory systems in search
of its cause(s). Relatively recent novel ideas of stuttering
being a disorder of language planning surfaced (e.g., Perkins,
Kent, & Curlee, 1991; Postma & Kolk, 1993), which in
turn gave rise to a number of investigations that have since
tested this prospect. To date, studies that examined the
relationship between stuttering and speech execution still con-
siderably outnumber those that investigated the precipitative

aState University of New York College at Cortland
Correspondence to Irena Vincent: irena.vincent@cortland.edu

Editor: Julie Liss
Associate Editor: Susanne Fuchs
Received August 9, 2016
Revision received March 14, 2017
Accepted May 13, 2017
https://doi.org/10.1044/2017_JSLHR-S-16-0323

role that language planning might play in its occurrence. In
order to redress this imbalance, the present study searched
for a possible causative relationship between deficits in
selected language planning processes and stuttering.

Transformation of a thought into a verbal message
requires completion of several covert operations: (a) con-
ceptualizing, that is, forming an idea of what is to be said;
(b) semantic and syntactic encoding, that is, selecting the
semantic and syntactic features (lemmas) of the intended
message; (c) phonological encoding, that is, retrieving phono-
logical properties for the selected lemmas; and (d) phonetic
encoding, that is, creating a motor plan to be executed by
the articulatory system (e.g., Levelt, 1989). Over the past
two decades, the sequential order in which these psycho-
linguistic processes occur in typical adult speakers has
been corroborated by multiple electrophysiological stud-
ies (e.g., Rodriguez-Fornells, Schmitt, Kutas, & Münte,
2002; Schmitt, Schiltz, Zaake, Kutas, & Münte, 2001;
van Turennout, Hagoort, & Brown, 1998). Concurrently,
hypotheses of stuttering that ascribed fluency breakdowns
to aberrant semantic and/or phonological encoding incited

Disclosure: The author has declared that no competing interests existed at the time
of publication.

Journal of Speech, Language, and Hearing Research (cid:129) Vol. 60 (cid:129) 2537–2550 (cid:129) September 2017 (cid:129) Copyright © 2017 American Speech-Language-Hearing Association 2537

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions research that sought to test their validity. The notion has
received equivocal support from investigations that admin-
istered standardized speech-language tests to children who
stutter and children who do not stutter and found either
diminished, though not necessarily subclinical, or compara-
ble language ability in children who stutter relative to chil-
dren who do not stutter (for a review, see Bloodstein &
Bernstein Ratner, 2008). Although depressed standardized
scores appear indicative of an underlying linguistic deficiency
in stuttering, they are too crude a measure to discern which
process(es) specifically might contribute to disfluent speech.
This type of scrutiny came from research that exposed
preschool and school-age children, as well as adults who
stutter (AWS), to an array of paradigms uniquely designed
to manipulate the semantic and phonological processing
demands of experimental tasks. As the present investigation
focused on individuals whose stuttering persisted beyond
childhood and whose language skills were mature, studies
that have examined semantic and phonological encoding
in adult stuttering contain the most pertinent background
information and are reviewed next.

Semantic Encoding and Stuttering

Empirical attempts to quantify and qualify semantic

encoding efficacy in AWS have been relatively sparse;
nevertheless, a fair number of available reports presented
data that implicate faulty semantic encoding as a plausible
catalyst for stuttering disfluencies. For example, Rastatter
and Dell (1987) observed a discrepancy between AWS and
adults who do not stutter (ANS), who phonated the vowel
ah in response to a visually presented concrete word and
remained silent when presented with a nonsense letter string.
The task proved more challenging for AWS as they were
discernibly slower than ANS in providing the required
response. Bosshardt and Fransen (1996) and Bosshardt,
Ballmer, and De Nil (2002) devised a category-monitoring
task to compare semantic encoding abilities between AWS
and ANS. When instructed to read sentences silently and
identify the superordinate category for target words embed-
ded in the text, AWS were slower than ANS in making the
semantic decision (Bosshardt & Fransen, 1996). Under the
conditions of overtly constructing sentences while simulta-
neously retrieving category information, the groups did not
differ in the latency of category identification; however,
the adverse impact of the semantic task on sentence gen-
eration proficiency was more pronounced in AWS than in
ANS (Bosshardt et al., 2002). The findings were taken as
evidence of deficient semantic encoding, which may be
slower or at least more likely to interfere with sentence
planning in AWS.

Subsequent studies used auditory semantic priming

to continue this line of research. In a study by Hennessey,
Nang, and Beilby (2008), AWS and ANS listened to cate-
gorically related primes presented before or after picture
stimuli whose labels were high- and low-frequency words.
Regardless of prime presentation time and target word
frequency, semantic priming had the same inhibiting effect

on picture naming in both groups. Maxfield, Huffman,
Frisch, and Hinckley (2010) and Maxfield, Pizon-Moore,
Frisch, and Constantine (2012) relied primarily on brain
physiology in their assessment of semantic processing in
AWS. They recorded event-related potentials and response
accuracy during picture naming primed by semantically
unrelated and semantically related words. The studies pro-
duced offsetting behavioral data, as overt picture naming left
AWS indistinguishable from ANS in Maxfield et al. (2010)
but rendered them less accurate than ANS in Maxfield et al.
(2012). Electrophysiologically, however, semantic priming
differentially affected the groups on both occasions, as
semantic activation decreased for related versus unrelated
probes in ANS but increased (Maxfield et al., 2010) or
remained the same (Maxfield et al., 2012) in AWS. It is plau-
sible that AWS differ from ANS in the way they process
semantic information even when this difference cannot be
detected in their overt speech production.

Phonological Encoding and Stuttering

As a potential etiological factor of stuttering, phono-

logical encoding has drawn more attention than semantic
encoding, thereby instigating appreciably more research.
Not unexpectedly, the higher number of conducted studies
embodies a correspondingly greater variety of employed
methodologies and a greater heterogeneity of obtained data.
One of the earliest approaches used to explore phono-

logical encoding in persons who stutter is phonological
priming. The method typically involves facilitating overt
speech by supplying the speaker with one or more phonemes
of the target utterance. In the Wijnen and Boers (1994)
investigation, adults produced a series of responses following
visually presented cue words, with the response sequences
consisting of words that shared no initial phonemes, words
that shared the initial consonant, or words that shared the
initial consonant and vowel. In all conditions, AWS were
slower than ANS. In addition, the pattern of changes in
response speed due to phonological priming differed for the
groups: In ANS, speech onset latency decreased in inverse
proportion to the number of shared phonemes, whereas in
AWS, latencies did not decrease until both the onset and
nucleus of the response words were provided. According to
the authors, this combination of between- and within-group
differences attested to the slowness of phonological encoding
in stuttering. In their attempts to replicate the study, Burger
and Wijnen (1999) and Vincent, Grela, and Gilbert (2012)
also observed AWS to be generally slower than ANS; how-
ever, they were unable to attribute this finding to delayed
phonological encoding because both groups responded to
priming identically by becoming faster with each provided
phoneme. Hennessey et al. (2008) did not corroborate Wijnen
and Boers’ (1994) data either. They used overt picture nam-
ing auditorily primed by words phonologically unrelated
or related to the target utterance and found that both
AWS and ANS became faster when responding to related
primes. Nonetheless, by recording brain potentials during
the same task, Maxfield et al. (2012) captured atypical

2538 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 60 (cid:129) 2537–2550 (cid:129) September 2017

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions priming responses in AWS: While phonological activation
decreased for related versus unrelated probes in ANS, it
increased in AWS.

Alongside priming, rhyme and phoneme monitoring

have also been recognized as tasks that place demands
specifically on phonological encoding and, therefore, suit-
able for the detection of phonological encoding difficulties.
Under this premise, Bosshardt and Fransen (1996) directed
AWS and ANS to read cue words silently and search for
rhyming targets in subsequently presented sentences. The
groups demonstrated similar speed of rhyme identification,
hence negating the existence of slower phonological encod-
ing in adult stuttering. In Bosshardt et al. (2002), AWS
and ANS made rhyme decisions during overt sentence pro-
duction. Participants were comparable in how quickly they
retrieved the required phonological information, but AWS
composed less complex sentences than ANS, implicating
phonological encoding as a conceivable hindrance to sen-
tence formulation. Weber-Fox, Spencer, Spruill, and Smith
(2004) drew a somewhat analogous inference from behav-
ioral and electrophysiological data produced by AWS and
ANS as they made silent rhyme judgments for ortho-
graphically similar and dissimilar word pairs. Between-
group comparisons revealed no differences in manual
response accuracy or in brain response latency and ampli-
tude, whereas within-group comparisons led to significant
findings. Relative to other rhyme-orthography combinations,
word pairs that did not rhyme but that looked similar
required a longer processing time in AWS than in ANS.
In addition, AWS activated the right hemisphere more than
the left during rhyme monitoring, with no signs of hemi-
spheric asymmetry observed in ANS. The authors proposed
that greater vulnerability to increased cognitive load, rather
than a phonological encoding deficit, may be an under-
lying component of adult stuttering. The conjecture was
validated by Jones, Fox, and Jacewicz (2012), who required
AWS to memorize letter strings of varied length while
completing a rhyme-judgment task identical to the one in
Weber-Fox et al. (2004). Regardless of the complexity of
rhyme-orthography and memory conditions, AWS did not
differ from ANS in rhyme identification accuracy. Rhyme
identification latency, however, differentiated one group
from the other: Not only were AWS slower than ANS, and
especially so when word pairs did not rhyme while looking
similar, but, unlike their fluent peers, they also responded
with different speed across different complexity levels of the
concurrent memory task. Sasisekaran and De Nil (2006)
and Sasisekaran, De Nil, Smyth, and Johnson (2006) relied
on phoneme monitoring during silent picture naming to
uncover whether AWS process phonological information
differently from ANS. Both studies confirmed that this
might indeed be the case as AWS retrieved the target pho-
nemes more slowly than ANS.

Nonword repetition entails, among other steps, the
storage and retrieval of novel sound sequences, which is
why it has been administered to individuals who stutter to
gain further insight into their ability to process phono-
logical information. Smith, Sadagopan, Walsh, and Weber-

Fox (2010); Byrd, Vallely, Anderson, and Sussman (2012);
Sasisekaran (2013); Sasisekaran and Weisberg (2014); and
Byrd, McGill, and Usler (2015) found AWS to resemble
ANS in the accuracy with which they repeated English-like
nonwords of relatively short one- to four-syllable length,
yet differ from them on other variables of interest. Specifi-
cally, AWS lagged behind ANS in the accuracy of produc-
ing lengthier and phonemically more complex nonwords
(Byrd et al., 2015; Byrd et al., 2012; Sasisekaran, 2013;
Sasisekaran & Weisberg, 2014) and in the consistency
of lip movement coordination across three-, four-, six-,
and 11-syllable stimuli (Sasisekaran, 2013; Sasisekaran &
Weisberg, 2014; Smith et al., 2010). This led the authors
to consider the idea of phonological processing tampering
with fluent speech production in AWS.

Purpose of the Present Study

Semantic and phonological encoding, along with

other premotor stages, are preconditions to speech motor
planning and execution (e.g., Levelt, 1989) and have been
suspected as having a causal role in stuttering. Under this
construct, it seems reasonable to assume that one faulty
link (e.g., semantic encoding) in this sequence of linguistic
events might also antecede a disturbance in each successive
process (e.g., phonological encoding) before culminating
in an overt fluency disruption (e.g., Hennessey et al., 2008).
According to the preceding review, there appears to be
partial agreement among the studies that AWS are not en-
tirely free from some degree of semantic encoding difficulty;
the scarcity of presently available findings, however, renders
this agreement rather more tentative than definitive. The
explorations of phonological encoding in individuals who
stutter are more abundant but nonetheless offer just as little
assurance and much contending evidence regarding the
alleged link between this process and disfluent speech. Poten-
tial origins of these tentative and diverging reports may lie,
at least partly, in one of the following methodological limita-
tions. First, although analogous in their main purpose, the
conducted studies were less uniform in their experimental
paradigms as they used a variety of tasks, including semantic
priming, category monitoring, phonological priming, pho-
neme monitoring, rhyme monitoring, and nonword repetition.
Each task may be targeting a different aspect of semantic
and phonological encoding (e.g., segmentation for phoneme
monitoring vs. storage and retrieval of novel sound sequenc-
ing for nonword repetition), thereby producing differing
results. Second, these paradigms may not have successfully
disentangled the covert process of interest from other pre-
ceding and/or succeeding processes involved in the planning
and execution of the required response. For example, the
majority of protocols required that participants produce
overt speech (e.g., Burger & Wijnen, 1999), process auditory
cues (e.g., Byrd et al., 2015; Hennessey et al., 2008), and/or
read single words or sentences (e.g., Bosshardt et al., 2002;
Vincent et al., 2012), with each of these requirements possi-
bly contaminating the results. Therefore, when significant
between-group differences were found, the exact source

Vincent: Semantic and Phonological Encoding in Stuttering 2539

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions of the difference could not always be located. Third, a
fair number of studies included participants who stutter of
unknown (e.g., Hennessey et al., 2008) or, as Byrd et al.
(2012) noted, mild or very mild severity rating (e.g., Smith
et al., 2010; Weber-Fox et al., 2004) and produced negative
findings. If the degree of stuttering severity, as measured
by overt manifestations of the disorder, is commensurate
with the degree of the underlying difficulty, research results
may be contingent to some extent on the severity makeup
of experimental groups, with subtle symptoms conceivably
impeding the discovery of group differences.

The aforementioned factors create a need not only for

further research on semantic and phonological encoding in
stuttering but also for continued refinement of the method-
ology used to isolate, manipulate, and measure these linguis-
tic processes in speakers susceptible to fluency breakdowns.
Thus, the purpose of the present study was to gather addi-
tional data on semantic and phonological encoding abilities
in AWS by employing an experimental protocol that does
not require overt speech, auditory processing, or reading as
part of participants’ responses. Specifically, AWS and ANS
silently identified and manually indicated the superordinate
category (animal vs. object) and the initial phoneme (vowel
vs. consonant) of pictorial stimuli as a means of demon-
strating their accuracy and speed of semantic and phono-
logical encoding, respectively (e.g., Rodriguez-Fornells et al.,
2002). Baseline manual response time (RT) was obtained
to account for a potential confounding effect of the latency
of movement execution on the latency of semantic and
phonological decisions (e.g., Sasisekaran et al., 2006). In
addition, the AWS participants overtly presented disfluency
levels that were predominantly moderate but ranged from
very mild to very severe (Byrd et al., 2012). To achieve the
principal aim of the study and discover whether deficits in
semantic or phonological encoding might be a hindrance to
fluent speech, the following questions were posed: (a) Are
AWS slower or less accurate than ANS when silently identi-
fying picture stimuli as animals/objects and (b) are they
slower or less accurate than ANS when silently identifying
initial vowels/consonants during presentation of picture
stimuli? It was hypothesized that, if either semantic or phono-
logical encoding is aberrant in stuttering, affirmative evidence
would be found, with AWS demonstrating significantly
greater difficulty than ANS in category and/or sound
identification.

Methods

The study was approved by the State University
of New York College at Cortland Institutional Review
Board, and written informed consent was obtained from
all participants.

Participants

Participants were monolingual native speakers of

English, who were naïve to the purpose of the study. Two
age- and sex-matched groups completed the experimental

tasks: 15 AWS, ranging in age from 20 to 62 years (mean
age = 32;6), and 15 ANS, ranging in age from 21 to 60 years
(mean age = 32;7). Eleven men and four women were in
each group. The age difference between an AWS and the
control ANS ranged from 0 years and 1 month to 2 years
and 0 months (mean age difference = 0;10). A one-way
analysis of variance (ANOVA) did not reveal a significant
between-group difference in chronological age, F(1, 28) =
0.001, p = .976, ηp
2 < .001. Apart from stuttering in the
AWS group, none of the participants disclosed present or
past speech, language, hearing, or neurological problems,
and all had normal or corrected-to-normal vision. Ten
AWS and one ANS reported a family history of stuttering.
Twelve AWS reported an increased likelihood of stuttering
on specific sounds or sound groups. All AWS underwent
one to four courses of therapy for the disorder during child-
hood and/or adulthood, with none receiving treatment at
the time of the study. These treatment approaches included
stuttering modification, fluency shaping, breathing/voice/
articulation exercises, medications, relaxation, hypnotherapy,
and acupuncture. Currently, there is no reported evidence
of the effect these methods may have on the speed of seman-
tic and phonological encoding.

The classification of participants as AWS and ANS

was based on stuttering severity and stuttering history. The
Stuttering Severity Instrument for Children and Adults–
Third Edition (SSI-3; Riley, 1994) was used to determine
stuttering severity. A 500-syllable spontaneous speech sample
was obtained from a conversation that each participant
had with the author, and a 228-syllable reading sample
was obtained from the participant’s reading of an age-
appropriate text (SSI-3 Plate XI: Reading, adult level). These
were analyzed for within-word disfluencies, including sound
and syllable repetitions, audible and inaudible sound pro-
longations, and monosyllabic whole-word repetitions. Indi-
viduals included in the AWS group scored at least “very
mild” on the SSI-3, which corresponded to an overall score
of 10 or higher. In addition, they reported that their stutter-
ing began in childhood and persisted into adulthood. Indi-
viduals included in the ANS group scored below “very
mild” on the SSI-3 and reported no history of childhood
or adult stuttering. The SSI-3 score ranged from 12 to
45 (mean score = 26.8) for AWS and from 0 to 6 (mean
score = 1.5) for ANS. A one-way ANOVA revealed a
significant between-group difference in stuttering severity,
F(1, 15.672) = 122.755, p < .001, ηp
of all participants is presented in Table 1.

2 = .814. A description

Interjudge and intrajudge reliability measurements
were determined for stuttering disfluencies. For five ran-
domly selected AWS, the 500-syllable conversational and
228-syllable reading samples were analyzed, resulting in
a total of 3,640 syllables used in reliability assessment. To
determine intrajudge reliability, the author completed two
disfluency analyses: (a) initial analysis of the speech samples
produced by the five AWS participants and (b) analysis of the
same samples 6 months after the initial analysis. To deter-
mine interjudge reliability, another certified speech-language
pathologist rated the same speech samples re-analyzed

2540 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 60 (cid:129) 2537–2550 (cid:129) September 2017

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Table 1. Participants’ sex, age (years;months), family history, SSI-3 score, stuttering severity, reported time of stuttering onset, and sounds
associated with an increased likelihood of stuttering.

Participant

Sex

Age

Family
history

SSI-3
score

Stuttering
severity

Reported time of
stuttering onset

Sounds associated
with stuttering

Mild
Moderate
Moderate
Moderate
Moderate
Severe
Moderate
Very severe
Very Severe
Very mild
Severe
Mild
Very mild
Moderate
Mild

“5 years old”
“as long as I can remember”
“starting school”
“age 4”
“I have always stuttered”
“childhood, lifelong stutter”
“all my life”
“since I began talking”
“at age 5”
“5 or 6 years old”
“1st–2nd grade”
“when I was a little kid”
“as long as I can remember”
“as long as I can remember”
“early childhood”

“hard sounds”
None specified
/l/, vowels
Word-initial /m/, /n/
Plosives, /v/
/b/, /k/, /p/, /ʃ/
None specified
“any hard sound”
/b/, /h/, /k/, /l/, /p/
Word-initial /b/, /d/
/b/, /m/, /n/, /p/, /w/, /z/
Word-initial /h/, /r/, /s/
Word-initial /m/, /w/
None specified
Word-initial “hard

consonants and vowels”

AWS1
AWS2
AWS3
AWS4
AWS5
AWS6
AWS7
AWS8
AWS9
AWS10
AWS11
AWS12
AWS13
AWS14
AWS15

ANS1
ANS2
ANS3
ANS4
ANS5
ANS6
ANS7
ANS8
ANS9
ANS10
ANS11
ANS12
ANS13
ANS14
ANS15

Male
Male
Female
Male
Male
Female
Male
Female
Male
Female
Male
Male
Male
Male
Male

Male
Male
Female
Male
Male
Female
Male
Female
Male
Female
Male
Male
Male
Male
Male

25;1
36;8
62;2
23;8
27;2
44;10
21;3
20;7
20;7
45;5
40;3
24;8
22;2
20;1
52;10

24;10
37;6
60;2
23;9
27;7
42;10
21;11
21;2
21;3
47;0
42;1
24;2
22;7
20;7
52;3

Yes
Yes
Yes
Yes
Yes
Yes
No
Yes
Yes
No
No
No
Yes
Yes
No

No
No
No
Yes
No
No
No
No
No
No
No
No
No
No
No

24
26
28
26
27
33
26
40
45
12
33
19
17
27
19

4
0
0
4
0
0
2
0
0
0
0
6
4
3
0

Note. SSI-3 = Stuttering Severity Instrument for Children and Adults–Third Edition; AWS = adults who stutter; ANS = adults who do not
stutter.

by the author. (Dis)fluency counts provided by each exam-
iner were inserted in the formula ([A + B]/[A + B + C + D])
× 100, in which A, B, C, and D represented the number of
syllables judged disfluent on both occasions, the number
of syllables judged fluent on both occasions, the number of
syllables judged disfluent on the first but not the second
occasion, and the number of syllables judged disfluent
on the second but not the first occasion, respectively (e.g.,
Arnold, Conture, & Ohde, 2005). Intrajudge agreement
for stuttering disfluencies was 98%; interjudge agreement
for stuttering disfluencies was 96%.

Stimuli

Two types of visual stimuli—neutral and pictorial—
were used in neutral, semantic encoding, and phonological
encoding experimental tasks. The neutral stimulus was an
asterisk presented in the center of the computer screen with
font type Arial, size 2.5 cm × 2.5 cm, font color white,
background color black. Pictorial stimuli were 100 clipart
images in color presented against the black background
and as similar as possible in style and size. Depending on
the natural shape of the depicted item, picture dimensions

were approximately 8–14 cm in width and 7–11 cm in height.
These were chosen from a set of 140 pictures, which had
been compiled in a booklet and then presented to and named
in writing by 10 adult volunteers that did not participate in
the main study. The 100 pictures used in the experimental
conditions had been assigned the same label by at least eight
volunteers and had a mean naming agreement of 97.2%.
Half of the selected images served as stimuli in the semantic
encoding task, with 25 depicting animals and 25 depicting
objects. The other half served as stimuli in the phonological
encoding task, with 25 depicting items whose target labels
begin with a vowel and 25 depicting items whose target
labels begin with a consonant. All pictures corresponded
to mono- or bisyllabic target labels, which represented low-,
medium-, and high-frequency American English nouns
(see the Appendix for a list of all target labels). Word fre-
quency, as determined by the CELEX linguistic database
(Max Planck Institute for Psycholinguistics, 2001), was sub-
jected to a one-way ANOVA with experimental condition
(animal, object, vowel, consonant) serving as the between-
group factor. The animal, object, vowel-initial, and consonant-
initial labels had an average frequency of 28, 47, 38, and
34 occurrences per million, respectively. The analysis did

Vincent: Semantic and Phonological Encoding in Stuttering 2541

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions not reveal significant differences in average word frequency
values among the four conditions, F(3, 96) = 0.244, p = .866,
ηp

2 = .008.

Instrumentation and Procedure

The participants were tested in a dimly lit and quiet

lab space in order to minimize visual distractions and
ambient noise. They were seated in a height-adjustable chair
at an L-shaped built-in corner work surface. A laptop with
E-Prime v.2.0 software package (Psychology Software
Tools, Inc., 2012) and a Serial Response Box (Psychology
Software Tools, Inc., Pittsburgh, PA) with a row of five
response buttons were used to control the presentation of
the visual stimuli and the acquisition of the participants’
dominant index finger RT. The laptop was placed in front
of the participant, with the 15-in. screen at a viewing distance
of approximately 1 m and at an optimal visual angle as
determined and adjusted by each participant. The Serial
Response Box was placed on the work surface lateral to
the dominant hand, with the first button in the row serving
as the response button for right-handed participants and
the last button in the row serving as the response button
for left-handed participants. Throughout the experiment,
the participants comfortably rested their dominant arm and
hand on the adjacent work surface and their index finger on
the response button. The latency of index finger responses
was measured in milliseconds from the onset of the target
stimulus presentation to the button-press response.

All experimental tasks were completed in one 50- to

60-min session, with a short rest period given when transi-
tioning from the first task to the second and from the second
to the third. The order of neutral, semantic encoding, and
phonological encoding tasks was randomized across partici-
pants to account for the possible confounding effect of
fatigue on RT.

Neutral Task

The neutral task served to assess the latency of

index finger responses (i.e., movement execution) when
no language processing was required. All participants were
instructed to remain silent during the task and press the
response button as quickly as possible upon presentation of
the neutral stimulus. The asterisk appeared at the center of
the screen for a predetermined maximum time of 2,000 ms.
If the participant responded in less than 2,000 ms, the
stimulus disappeared from the screen as soon as the button
was pressed. The asterisk was presented 50 times at random
intertrial intervals (i.e., blank screen between the end of
one trial and the beginning of the following trial), which
ranged in duration from 1,000 to 7,000 ms. Every participant,
therefore, had the possibility of producing a maximum of
50 required responses.

Semantic Encoding Task

Semantic encoding was assessed by silently determin-
ing the superordinate word category (animal vs. object) for
animate and inanimate images. Two semantic subtasks

were completed: (a) animal subtask, during which the par-
ticipants pressed the response button as soon as they iden-
tified the presented picture as an animal (e.g., dolphin,
frog, turtle) and withheld the response if they identified it
as an object (e.g., fork, glasses, toaster); and (b) object subtask,
during which the participants pressed the response button as
soon as they identified the presented picture as an object
and withheld the response if they identified it as an animal.

Prior to completing the subtasks, participants took

part in familiarization, naming, and practice sessions (e.g.,
Hennessey et al., 2008; Rodriguez-Fornells et al., 2002;
Sasisekaran et al., 2006; Schmitt et al., 2001). The comple-
tion time for this part of the protocol did not exceed 10 min.
They became familiar with all stimuli and target labels by
silently viewing each of the 50 animal and object pictures
with its name displayed underneath in white uppercase let-
ters, font type Arial, size 1.5 cm. The familiarization session
was self-paced, and participants had as much time as they
needed to study each image and its name before pressing
the response button to proceed to the next image. During
the subsequent naming session, the same pictures were dis-
played on the screen but without their names. Participants
overtly named each image as well as identified it as an
animal or object to demonstrate the knowledge of designated
target labels and superordinate categories. The productions
were recorded by a digital voice recorder (WS-100, Olympus
Corporation of the Americas, Center Valley, PA) and later
inspected for naming and category errors. As with familiar-
ization, naming was also self-paced: Participants had as
much time as needed to produce their verbal responses
before pressing the response button to proceed to the next
image. This course was decided upon because it would
have been possible for AWS to know the required infor-
mation but nonetheless produce an absent or incomplete
response owing to a disfluency that is longer than a preset
time frame. As such answers would have underestimated
their subtask-specific language abilities, no RT limit was
imposed to ensure a valid assessment of target label and
category knowledge. During the practice session, the condi-
tions of a semantic subtask were simulated by presenting
10 stimuli, five depicting animals and five depicting objects.
The practice stimuli were different from the ones used in the
semantic subtasks, and responses to them were not included
in the final data analysis. The order of picture presenta-
tion in all three sessions was randomized by the E-Prime
software.

Following familiarization, naming, and practice ses-

sions, participants completed the animal and object subtasks.
The order of subtask completion was counterbalanced across
participants: In each group, eight participants completed
the animal and seven completed the object subtask first. All
participants were instructed to remain silent during the sub-
tasks and to execute the response as quickly as possible upon
presentation of a stimulus. Prior to the presentation of each
stimulus, a fixation point (+) appeared at the center of the
screen for 500 ms. After this time, the fixation point dis-
appeared, and the screen remained blank for 500 ms before
the picture was presented for a predetermined maximum

2542 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 60 (cid:129) 2537–2550 (cid:129) September 2017

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions time of 2,500 ms. If the participant responded in less than
2,500 ms, the picture disappeared from the screen as soon
as the response button was pressed. The following fixation
point was presented after the intertrial interval of 1,000 ms.
In each subtask, the 25 animal and 25 object images were
presented twice in order to increase the total number of
responses. Every participant, therefore, had the possibility
of correctly executing the response 50 times and correctly
withholding the response 50 times per subtask. The order in
which the pictures were presented was randomized by the
E-Prime software.

Phonological Encoding Task

Phonological encoding was assessed by silently deter-
mining the initial sound type (vowel vs. consonant) for the
target labels of the presented images. Two phonological
subtasks were completed: (a) vowel subtask, during which
the participants pressed the response button as soon as
they identified that the target label of the presented picture
began with a vowel (e.g., arrow, egg, iron) and withheld
the response if the label began with a consonant (e.g., book,
drum, shoe); and (b) consonant subtask, during which the
participants pressed the response button as soon as they
identified that the target label of the presented picture
began with a consonant and withheld the response if the
label began with a vowel.

Prior to completing the subtasks, the participants
took part in familiarization, naming, and practice sessions,
which were executed in the same manner as in the semantic
encoding task. The only difference was in the 50 pictures
used in the phonological encoding task. Thus, during famil-
iarization, 50 images whose target labels began with a vowel
or a consonant were displayed. Similarly, during naming,
the participants overtly named each image as well as identi-
fied its initial sound as a vowel or consonant in order to
demonstrate a knowledge of the designated target labels
and their initial sounds. Finally, during practice, the condi-
tions of a phonological subtask were simulated by presenting
10 stimuli, five depicting images with word-initial vowels
and five depicting images with word-initial consonants.
Following familiarization, naming, and practice
sessions, participants completed the vowel and consonant
subtasks. The order of subtask completion was counter-
balanced across participants: In each group, eight participants
completed the vowel, and seven completed the consonant
subtask first. With the exception of the pictures used, the
phonological subtask design was identical to that of the
semantic subtask.

Dependent Variables
Overt Naming, Category Identification,
and Sound Identification Accuracy

Eight dependent variables were measured and analyzed

for the overt responses produced by participants during the
semantic and phonological naming sessions: (a) animal picture
naming errors, (b) object picture naming errors, (c) total
animal and object picture naming errors, (d) superordinate

category identification errors, (e) vowel-initial picture naming
errors, (f ) consonant-initial picture naming errors, (g) total
vowel- and consonant-initial picture naming errors, and
(h) initial sound identification errors. A picture naming error
was a response that was absent, incomplete, or different in
any way from the target label presented during the familiar-
ization session; a category identification error referred to
classifying an object as an animal or an animal as an object;
a sound identification error referred to mistaking a word-
initial vowel for a consonant or a word-initial consonant
for a vowel. Each participant had to produce 80% or more
correct responses during overt naming and category/sound
identification to be included in the final data analysis. This
accuracy score was taken as evidence that participants
accessed both the correct target labels and semantic and
phonological information during the semantic and phono-
logical encoding subtasks when no overt speech responses
were required. Between- and within-group comparisons of
the frequency of each error type were performed to evaluate
whether AWS and ANS possessed similar language knowl-
edge required for completion of the experimental tasks.

Silent Neutral, Semantic Encoding, and Phonological
Encoding Accuracy and RT

Seven types of errors and seven types of RTs were the

dependent variables measured and analyzed for the data
obtained during the silent neutral, semantic, and phono-
logical tasks.

The responses that were coded as errors and excluded

from RT analyses were incorrect decisions made when no
language processing was required (i.e., during the neutral
condition), when semantic encoding was required (i.e., dur-
ing the animal and object subtasks), and when phonological
encoding was required (i.e., during the vowel and consonant
subtasks). An incorrect response was a button press that
was executed instead of withheld (e.g., pressing the response
button upon object presentation during the animal subtask)
and a button press that was withheld instead of executed
(e.g., not pressing the response button upon object presen-
tation during the object subtask). Participants had to pro-
duce 80% or more correct responses in each of the five
experimental conditions to be included in the final data
analysis. Seven types of errors were examined: (a) neutral
task, (b) animal subtask, (c) object subtask, (d) total semantic
encoding, (e) vowel subtask, (f ) consonant subtask, and
(g) total phonological encoding. The frequency of each error
type was compared between the two groups with the primary
purpose of evaluating the speed–accuracy trade-off. Indi-
viduals can manipulate their response speed or response
accuracy when either speed or accuracy is a measure of inter-
est (Osman et al., 2000). Only if both groups had a compara-
ble frequency of incorrect responses could valid inferences
about their RT be made. If AWS produced significantly
more incorrect responses but the groups had similar RT,
this might be an indication of AWS trading accuracy for
speed, rather than AWS having response latency similar
to that of ANS.

Vincent: Semantic and Phonological Encoding in Stuttering 2543

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions RT was recorded by E-Prime during neutral, animal,

object, vowel, and consonant conditions. RT was defined
as the time interval between the onset of the visual stimulus
and the onset of the participant’s button-press response.
Seven types of RT were examined: (a) neutral, (b) animal,
(c) object, (d) overall semantic encoding, which was cal-
culated by averaging animal and object RTs, (e) vowel,
(f ) consonant, and (g) overall phonological encoding, which
was calculated by averaging vowel and consonant RTs.

Results

Obtained data were analyzed using the SPSS v.19.0

statistical software package (SPSS Corporation, Chicago, IL).
A significance level of .05 was used for all statistical analyses.
Partial eta squared (ηp
2) is reported for each between- and
within-group comparison as an indicator of a small (.01 ≤
ηp
effect size.

2 < .06), medium (.06 ≤ ηp

2 < .14), or large (.14 ≤ ηp

2)

Overt Naming, Category Identification,
and Sound Identification Accuracy

All participants produced 80% or more correct

responses during overt naming and category and sound
identification. For both groups, the percentage of errors
made during the naming sessions ranged from 0% to 10%.
The data related to the mean number of specific error
types made by AWS and ANS during the naming sessions
are presented in Table 2.

A mixed 2 × 2 ANOVA was performed with speaker

group (AWS, ANS) as the between-subject factor and
category (animal, object) as the within-subject factor. No

Table 2. Mean values for the errors produced by AWS and ANS
during the overt semantic encoding and phonological encoding
naming sessions.

Error

Overt semantic encoding
naming session
Animal picture naming
Object picture naming
Total animal and object

picture naming
Category identification
Overt phonological encoding

naming session

Vowel-initial picture naming
Consonant-initial picture

naming

Total vowel- and consonant-

initial picture naming

Sound identification

AWS

ANS

Mean

SD

Mean

SD

0.8
0.8
1.6

0.0

0.6
0.6

1.2

0.0

0.7
0.8
0.7

0.0

0.8
1.1

1.9

0.0

0.6
0.4
1.0

0.0

0.6
0.3

0.9

0.0

0.7
0.7
1.4

0.0

1.1
0.6

1.6

0.0

Note. AWS = adults who stutter; ANS = adults who do not
stutter.

significant main effect was found for either speaker group,
F(1, 28) = 2.250, p = .145, ηp
2 = .074, or category, F(1, 28) =
0.318, p = .577, ηp
between speaker group and category was not significant,
F(1, 28) = 0.318, p = .577, ηp
2 = .011. These findings indi-
cate that AWS and ANS were comparable in the accuracy
of overt animal and object picture naming.

2 = .011. In addition, the interaction

A separate mixed 2 × 2 ANOVA was performed

with speaker group (AWS, ANS) as the between-subject
factor and initial sound (vowel, consonant) as the within-
subject factor. The findings resembled those reported for
the animal and object pictures in that no significant main
effect was found for either speaker group, F(1, 28) = 0.175,
p = .679, ηp
ηp
2 = .034. Also, the interaction between speaker group
and sound was not significant, F(1, 28) = 1.000, p = .326,
ηp
2 = .034. These results suggest that AWS and ANS were
comparable in the accuracy of overt vowel- and consonant-
initial picture naming.

2 = .006, or sound, F(1, 28) = 1.000, p = .326,

The groups did not significantly differ in the accuracy
of verbal category and sound identification either; however,
the statistical procedure was not conducted because both
groups performed at ceiling and produced equal mean and
SD error values of zero.

These results suggest that the present AWS and ANS
did not significantly differ in their knowledge of designated
target labels, superordinate categories, and initial sounds
for the pictures used in the present study. It may further
be assumed that participants accessed correct target labels
and semantic and phonological information during semantic
and phonological encoding subtasks when no overt speech
responses were required.

Silent Neutral, Semantic Encoding, and
Phonological Encoding Accuracy and RT
Accuracy

All participants produced 80% or more correct
responses during the silent neutral, animal, object, vowel,
and consonant conditions. For AWS, the percentage of
errors made during the experimental tasks ranged from 0%
to 8%, whereas for ANS, it ranged from 0% to 9%. All
participants were therefore included in the final analysis.
The data related to the mean number of errors made by
AWS and ANS during each condition are presented in
Table 3.

During the neutral task, both groups performed at

ceiling, resulting in equal mean and SD error values of
zero. Therefore, although a statistical analysis could not
be performed with neutral task errors as the dependent
variable, it is apparent that the two participant groups did
not significantly differ in the accuracy of their responses to
the neutral stimulus.

To assess between- and within-group differences in

the number of errors made while making semantic decisions,
a mixed 2 × 2 ANOVA was performed with speaker group
(AWS, ANS) as the between-subject factor and category
(animal, object) as the within-subject factor. Although no

2544 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 60 (cid:129) 2537–2550 (cid:129) September 2017

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Table 3. Mean values for the errors and RT produced by AWS and
ANS during the silent neutral, semantic encoding, and phonological
encoding tasks.

Variable

Error

AWS

ANS

Mean

SD Mean

SD

Silent neutral task
Silent semantic encodinga

Animal subtask
Object subtask
Semantic encoding total
Silent phonological encodinga

Vowel subtask
Consonant subtask
Phonological encoding total

RT (ms)

Silent neutral task
Silent semantic encodinga,b

Animal subtask
Object subtask
Overall semantic encoding
Silent phonological encodinga,b

Vowel subtask
Consonant subtask
Overall phonological

encoding

0.00

0.00

0.00

0.00

0.27
1.20
1.47

1.33
2.53
3.86

0.46
1.21
1.30

2.02
2.56
4.22

0.13
0.60
0.73

1.20
2.07
3.27

0.35
0.82
1.00

0.94
2.74
3.28

292.1

14.9

276.2

30.8

443.7
476.2
459.9

678.8
732.3
705.6

59.0
55.9
52.5

79.8
97.8
84.8

398.9
440.7
419.8

640.9
633.7
637.3

42.9
51.0
41.8

73.2
74.1
67.7

Note. AWS = adults who stutter; ANS = adults who do not stutter;
RT = response time.
aSignificant within-group differences. bSignificant between-group
differences.

2 = .099, there was a significant main

significant main effect was found for speaker group, F(1, 28) =
3.080, p = .090, ηp
effect of category on accuracy, F(1, 28) = 12.600, p = .001,
ηp
2 = .310. The interaction between speaker group and
category was not significant, F(1, 28) = 1.400, p = .247,
ηp
2 = .048. These findings indicate that AWS and ANS were
comparable in the accuracy of their responses to animal and
object picture stimuli. Inspection of mean error values pro-
duced during the two semantic subtasks showed that both
groups produced significantly more errors when identifying
objects than when identifying animals.

A separate mixed 2 × 2 ANOVA was performed with

speaker group (AWS, ANS) as the between-subject factor
and initial sound (vowel, consonant) as the within-subject
factor to assess differences in the number of errors made
while making phonological decisions. While no significant
main effect was found for speaker group, F(1, 28) = 0.189,
p = .667, ηp
2 = .007, there was a significant main effect of
sound on accuracy, F(1, 28) = 6.788, p = .015, ηp
2 = .195.
The interaction between speaker group and initial sound was
not significant, F(1, 28) = 0.177, p = .786, ηp
2 = .006. These
findings suggest that AWS and ANS were comparable in the
accuracy of their responses to vowel- and consonant-initial
picture stimuli. Examination of mean error values produced
during the two phonological subtasks revealed that both
groups produced significantly more errors when identifying
consonants as the word-initial sound than when identifying
vowels as the word-initial sound.

RTs

For all between- and within-group analyses of RT,

errors were excluded, and only valid responses were considered.
In addition, valid RT values that were 2 SD below or
above the mean were removed from statistical analyses:
For the neutral task, 4.3% of trials were excluded for AWS
and 3.6% for ANS; for the semantic task, 4.1% of trials
were excluded for each group; for the phonological task,
5.4% trials were excluded for AWS and 4.8% for ANS.
Parametric statistics were used to analyze the remaining
valid responses as the Shapiro–Wilk test revealed that neu-
tral, semantic, and phonological RT data met the normality
assumption. The RT mean and SD values produced by
each group in the experimental conditions are presented
in Table 3.

To compare the groups on the basis of the speed of

their responses to the neutral stimulus, a one-way ANOVA
was performed with speaker group (AWS, ANS) as the
independent variable and neutral RT as the dependent var-
iable. Levene’s test of homogeneity of variance was signifi-
cant; therefore, the Welch test was used to compare the
group means. The test did not reveal a significant between-
group difference in neutral RT, F(1, 20.222) = 3.254, p = .086,
ηp
2 = .104, suggesting that AWS and ANS movement exe-
cution times were comparable when no language processing
was required.

To examine between- and within-group differences

in the speed of semantic encoding, a mixed 2 × 2 ANOVA
was performed with speaker group (AWS, ANS) as the
between-subject factor and category (animal, object) as the
within-subject factor. A significant main effect was found
for speaker group, F(1, 28) = 5.354, p = .028, ηp
2 = .161,
and category, F(1, 28) = 20.672, p < .001, ηp
2 = .425. The
interaction between speaker group and category was not
significant, F(1, 28) = 0.321, p = .575, ηp
2 = .011. These
findings indicate that AWS produced longer animal and
object RTs than ANS, which means that they were slower
in the speed of semantic encoding. Inspection of mean RT
values produced during the two semantic subtasks showed
that both groups were significantly slower when identifying
objects than when identifying animals.

To examine between- and within-group differences in
the speed of phonological encoding, a separate mixed 2 × 2
ANOVA was performed with speaker group (AWS, ANS)
as the between-subject factor and initial sound (vowel, con-
sonant) as the within-subject factor. There was a significant
main effect of speaker group, F(1, 28) = 5.930, p = .022,
ηp
2 = .175, and sound on RT, F(1, 28) = 5.037, p = .033,
ηp
2 = .152, as well as a significant interaction between
speaker group and sound, F(1, 28) = 8.610, p = .007, ηp
2 =
.235. To examine the significant interaction, a subsequent
independent t test was performed, which revealed that
AWS were slower than ANS when identifying consonants,
t(28) = 3.113, p = .004, but the groups were comparable
in the speed of vowel identification, t(28) = 1.356, p = .186.
In addition, a dependent t test showed that AWS identified
consonants as word-initial sounds slower than vowels, t(14) =
−3.747, p = .002, whereas ANS identified word-initial

Vincent: Semantic and Phonological Encoding in Stuttering 2545

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions consonants and vowels at similar speed, t(14) = .477,
p = .640.

Discussion

The goal of the present study was to investigate
whether speech fluency might be contingent on the speaker’s
ability to semantically and phonologically encode the
intended utterance. AWS and ANS were compared on the
basis of the accuracy and speed of category and phoneme
decisions, which were indicated manually during silent
viewing of pictorial stimuli. Movement execution latency
was accounted for, and no cognitive, linguistic, or motor
demands, such as those inherent to reading, listening, and
overt speaking, were posed on participants’ responses.
Therefore, category identification accuracy and speed were
considered indirect measures of semantic encoding efficiency
and sound identification accuracy and speed of phonological
encoding efficiency. The main findings and their implica-
tions are discussed next.

Semantic Encoding and Stuttering

To partake in the silent semantic subtasks, participants

had to demonstrate adequate requisite knowledge of the se-
mantic stimulus material. As evidenced by their performance
during the overt naming and category identification session,
AWS and ANS were equally and highly successful in retriev-
ing target picture names and classifying the stimuli as animals
or objects. The ensuing participation in the silent animal
and object subtasks generated the first main finding of this
investigation: AWS were slower but not less accurate than
ANS in semantic encoding, with objects eliciting more
errors and longer RTs than animals in both groups.

The significant between-group difference in semantic
encoding latency lends support to the hypothesis that seman-
tic encoding may be aberrant in AWS. As such, it diverges
from the null findings previously imparted by Hennessey
et al. (2008) and strengthens the affirmative research by
Bosshardt and Fransen (1996), Bosshardt et al. (2002), and
Maxfield et al. (2010, 2012), who also found semantic pro-
cessing differences between AWS and ANS. As already
noted, however, previous investigators uniformly created
conditions that activate multiple cognitive, linguistic, and
motor functions, thereby permitting the possibility that
the significant findings stemmed from processes other than
semantic encoding. The present probe design departed from
these methodologies because it was largely void of confound-
ing influences. Unlike natural speech acts, silent animate–
inanimate classification does not entail elaborate processing
and can be performed successfully by those who have a
known linguistic deficit. For example, individuals with ano-
mic aphasia have been found to execute silent categorization
tasks efficiently when target items share numerous dimen-
sions (e.g., items that are alive) but not when they share one
or very few dimensions (e.g., items that are yellow), suggest-
ing that high-dimensional categories require less language
support than low-dimensional ones (e.g., Lupyan & Mirman,

2013). Notwithstanding the simplicity of grouping together
stimuli that cohere in multiple dimensions, the current para-
digm was potent enough to augment latency values in AWS,
with the caveat of not pinpointing a specific cognitive–
linguistic origin of the performance breakdown. If replica-
ble, this positive finding might lead to inductive statements
about semantic encoding in AWS; if refined, this method-
ology might identify the weak component(s) within their
semantic system.

The comparison of the experimental subtasks uncov-

ered the within-group significant finding: In both groups,
category identification succeeding picture recognition was
less accurate and slower for inanimate than for animate
input. A category-specific performance dissociation is not
an atypical tendency: Whereas some research studies report
no category-specific (dis)advantage (e.g., Cappa, Perani,
Schnur, Tettamanti, & Fazio, 1998; Låg, 2005; Sitnikova,
West, Kuperberg, & Holcomb, 2006), some have documented
one semantic category faring better or worse than the other(s)
(e.g., Låg, 2005; Paz-Caballero, Cuetos, & Dobarro, 2006;
Proverbio, Del Zotto, & Zani, 2007), likely as a result of
varying experimental conditions. The present finding, which
revealed less efficient processing of stimuli belonging to the
object category, is compatible with studies that reported
an advantage of animals over objects (e.g., Proverbio et al.,
2007). It has been hypothesized that greater perceptual sim-
ilarity and predictability of animals, which all have heads
and eyes, is what makes them easier to categorize than per-
ceptually more diverse and unpredictable objects (e.g., Laws
& Neve, 1999).

Phonological Encoding and Stuttering

To engage in the silent phonological subtasks, partic-
ipants had to demonstrate adequate requisite knowledge of
the phonological stimulus material. As evidenced by their
performance during the overt naming and sound identifica-
tion session, AWS and ANS were equally and highly success-
ful in retrieving target picture names and classifying their
initial sounds as vowels or consonants. The ensuing partici-
pation in the silent vowel and consonant subtasks generated
the second main finding of this investigation: The groups
did not differ in phonological encoding accuracy, with con-
sonant errors outnumbering vowel errors in both groups,
and AWS were slower than ANS in consonant but not
vowel identification, with consonant RT lagging behind
vowel RT in AWS only. In other words, participants had
more difficulty identifying initial consonants than initial
vowels, but AWS were afflicted by the consonant disadvan-
tage more than ANS. This finding suggests that phono-
logical encoding may have been less efficient in the present
AWS than in ANS and that this deficiency may have been
sound-specific.

The first issue emerging from these results and warrant-

ing attention is that of a performance decline during con-
sonant relative to vowel identification. This phenomenon
varied in magnitude as a function of the fluency status
but was omnipresent and, thus, indicative of differential

2546 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 60 (cid:129) 2537–2550 (cid:129) September 2017

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions processing for the two sound types. As such, it coheres with
research on sound processing, which provides evidence
of consonant–vowel segregation in the brain. In adults who
are neurologically intact, consonants have been found to
evoke more activity in the frontal and vowels in the temporal
regions of the cortex (e.g., Carreiras & Price, 2008). Also,
in adults with neurological impairments, lesions in different
parts of the brain may cause one category of sounds to
be damaged independently of the other (e.g., Caramazza,
Chialant, Capasso, & Miceli, 2000). Along with differing in
the neural mechanisms they engage, consonants and vowels
also differ in the contribution they make to language. Func-
tionally, consonants carry more lexical information than
vowels (e.g., Nespor, Peña, & Mehler, 2003), which is the
most likely reason why word retrieval, recognition, and
reconstruction are more constrained under conditions of
omitted, transposed, or substituted consonants than vowels
(e.g., Carreiras, Gillon-Dowens, Vergara, & Perea, 2008;
Carreiras & Price, 2008; Cutler, Sebastian-Galles, Soler-
Vilageliu, & van Ooijen, 2000). This finding is independent
of the consonant–vowel (a)symmetry in a language: Whether
a language has many more consonants than vowels, such
as Spanish, or a balanced consonant–vowel ratio, such as
Dutch, consonant variations are more likely to trigger a
change in word meaning than vowel variations. Using this
knowledge to apprehend the phonological finding at hand
should be prefaced by noting that the present consonant and
vowel subtasks may at best be viewed as a crude extension
of the more suitable methods typically employed to exam-
ine differences between these sound classes. Hence, the intri-
cacies of the inhibitory effect of consonants on phonological
encoding in this study remain abstruse. This caveat aside,
the following provisional explanation may be considered. To
execute the task of classifying the initial sound, participants
first had to covertly retrieve the complete phonological form
for each target picture label. If it is more taxing to retrieve
lexicon-bearing consonants than prosody-bearing vowels, it
follows that participants would be less efficient when the
word-initial sound is a consonant than when it is a vowel.

The other point of discussion is that of AWS deterio-

rating not only in accuracy, like ANS, but also in latency
when identifying consonants versus vowels. This consonant-
biased difference has not been previously captured by studies
that supported the notion of a causative relationship between
diminished phonological abilities and stuttering and a plau-
sible source of this finding must be pondered. It is well estab-
lished that, alongside other linguistic factors, the initial
sound of the word is a veritable predictor of the location and
frequency of stuttering events (for a review, see Bloodstein
& Bernstein Ratner, 2008). In general, adult stuttering tran-
spires on consonants more than on vowels (e.g., Johnson &
Brown, 1935). Both the majority that conform to this rule
and the minority that depart from it are cognizant of their
pattern of disfluencies as they usually report difficulties with
specific sounds or sound groups. In this respect, AWS in
the present investigation were no exception: 10 individuals
reported various consonants as the sounds that occasion
stuttering, two reported a combination of consonants and

vowels, and three did not make note of a specific sound dif-
ficulty. Considering this, it cannot be ruled out that the un-
covered consonant disadvantage was an epiphenomenon
of the experimental group’s makeup. Put simply, had this
group been prone to stutter more on vowels instead of on
consonants, a contrasting finding might have emerged sug-
gesting that AWS are less efficient when encoding vowels
rather than consonants. In any case, this would suggest that
a flawed motor execution of a sound (i.e., a stuttering event)
may be traceable to faulty inner speech formulation (i.e.,
inefficient sound retrieval). However, further delineation of
the nature of these difficulties if or as they relate to specific
sound classes necessitates an experimental design that
would not only replicate the present use of mirroring vowel
and consonant tasks as one within-group factor but also
differentiate between individuals who stutter on consonants
and those who stutter on vowels as the other. Alternatively,
formulation of speech motor commands for the target sounds
(i.e., phonetic encoding) might have led to the present sig-
nificant finding (e.g., Brocklehurst & Corley, 2011). It is
probable that covert label retrieval took place subsequent
to phonological encoding and prior to the button press. This
means that, although no overt speech was required, the exe-
cuted manual response would not be impervious to the dif-
ficulties that AWS might have had with subvocal rehearsal
of the target picture labels.

Conclusions

The main semantic findings can be summarized
as follows: (a) AWS did not differ from ANS in semantic
encoding accuracy; (b) AWS produced slower RT than
ANS when categorizing animals and objects; and (c) both
AWS and ANS produced more errors and slower RT when
identifying objects than when identifying animals. This
is suggestive of the possibility that the semantic system of
AWS may be less robust than that of ANS. The main phono-
logical findings are the following: (a) AWS did not differ
from ANS in phonological encoding accuracy; (b) both
AWS and ANS produced more errors when identifying con-
sonants than when identifying vowels; (c) AWS were slower
than ANS in consonant but not vowel identification; and
(d) AWS were slower when identifying consonants than
when identifying vowels whereas ANS were not. Phono-
logical activation triggered by silent consonant–vowel
identification suggested that AWS might harbor a consonant-
specific phonological encoding weakness, which raised the
following question to be addressed by future research: Is
phonological encoding difficulty in AWS truly sound-specific,
and if so, is it a lexical, prosodic, or some other language
property the culprit that operates behind stuttered conso-
nants versus stuttered vowels?

Study Limitations and Implications
for Future Research

Timed behavioral data are reliant on the person’s
attention to task and cooperation in responding as quickly

Vincent: Semantic and Phonological Encoding in Stuttering 2547

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions as possible. At least four task and participant attributes
might have compounded this inherent drawback of the
present procedure and influenced response latencies across
participant groups and experimental conditions. First, the
target images were not selected from existing picture corpora
and, therefore, were not normed for either picture-name
correspondence or visual complexity (e.g., salience, color,
shape). Second, while care was taken to ensure that the
phonological subtasks had an equal number of consonant-
and vowel-initial stimuli, the consonantal and vocalic
onsets of the target labels were not controlled for. Conse-
quently, if covert rehearsal took place prior to the button
press and the initial sounds coincided with those likely
to be stuttered by AWS, the semantic and, perhaps more
importantly, phonological encoding results were confounded.
Third, only one response button was used to execute and
withhold a response, which resembles the go/no-go approach
typically employed by electrophysiological studies (e.g.,
Rodriguez-Fornells et al., 2002; van Turennout et al., 1998).
This simplified the decision-making process as it eliminated
the need to make finger or hand (i.e., button) decisions in
addition to making semantic and phonological decisions.
It should be noted, however, that this paradigm produces
relevant brain activation to be analyzed in electrophysio-
logical studies, whereas it precludes collection and analysis
of potentially relevant data (e.g., response bias) in behavioral
studies. Fourth, stuttering varies considerably in the time
of its onset, the nature of its evolvement, and the type and
severity of its overt and covert symptoms. The current
AWS had but two facets of the disorder in common: the
early childhood onset and persistence into adulthood.
Bar this similarity, the cohort was fairly heterogeneous as
exposed by self-reports on stuttering history and sound
difficulty and by formal analyses of overt stuttering severity.
On one hand, such a sample was representative of the adult
stuttering population, but on the other, preserving these
variations precluded a closer examination of specific stutter-
ing characteristics and their correlation to semantic and
phonological encoding latencies. For example, this investi-
gation rated overt stuttering and included all severity cat-
egories, but the potential effect of different degrees of
stuttering on the results could not be reliably examined
due to the relatively small sample size.

Given the limitations of the present design and the

discord between this and past methodologies, no perspective
on theoretical or clinical significance of the results can be
offered at this time. The present findings regarding semantic
and phonological inefficiency in AWS are positive, but only
future independent studies can discover if they are also rep-
licable and, potentially, a marker for persistent stuttering.

Acknowledgments

References
Arnold, H. S., Conture, E. G., & Ohde, R. N. (2005). Phonological
neighborhood density in the picture naming of young children
who stutter: Preliminary study. Journal of Fluency Disorders,
30, 125–148.

Bloodstein, O., & Bernstein Ratner, N. (2008). A handbook on stut-

tering (6th ed.). Clifton Park, NY: Thomson-Delmar.

Bosshardt, H.-G., Ballmer, W., & De Nil, L. F. (2002). Effects of
category and rhyme decisions on sentence production. Journal
of Speech, Language, and Hearing Research, 45, 844–857.
Bosshardt, H.-G., & Fransen, H. (1996). Online sentence process-
ing in adults who stutter and adults who do not stutter. Journal
of Speech and Hearing Research, 39, 785–797.

Brocklehurst, P. H., & Corley, M. (2011). Investigating the inner
speech of people who stutter: Evidence for (and against) the
Covert Repair Hypothesis. Journal of Communication Disorders,
44, 246–260.

Burger, R., & Wijnen, F. (1999). Phonological encoding and word
stress in stuttering and nonstuttering subjects. Journal of Fluency
Disorders, 24, 91–106.

Byrd, C. T., McGill, M., & Usler, E. (2015). Nonword repetition

and phoneme elision in adults who do and do not stutter: Vocal
versus nonvocal performance differences. Journal of Fluency
Disorders, 44, 17–31.

Byrd, C. T., Vallely, M., Anderson, J. D., & Sussman, H. (2012).

Nonword repetition and phoneme elision in adults who do and
do not stutter. Journal of Fluency Disorders, 37, 188–201.
Cappa, S. F., Perani, D., Schnur, T., Tettamanti, M., & Fazio, F.
(1998). The effects of semantic category and knowledge type
on lexical-semantic access: A PET study. Neuroimage, 8,
350–359.

Caramazza, A., Chialant, D., Capasso, R., & Miceli, G. (2000).

Separable processing of consonants and vowels. Nature, 403,
428–430.

Carreiras, M., Gillon-Dowens, M., Vergara, M., & Perea, M. (2008).
Are vowels and consonants processed differently? Event-related
potential evidence with a delayed letter paradigm. Journal of
Cognitive Neuroscience, 21, 275–288.

Carreiras, M., & Price, C. J. (2008). Brain activation for conso-

nants and vowels. Cerebral Cortex, 18, 1727–1735.

Cutler, A., Sebastian-Galles, N., Soler-Vilageliu, O., & van Ooijen,
B. (2000). Constraints of vowels and consonants on lexical
selection: Cross-linguistic comparisons. Memory & Cognition,
28, 746–755.

Hennessey, N. W., Nang, C. Y., & Beilby, J. M. (2008). Speeded
verbal responding in adults who stutter: Are there deficits in
linguistic encoding? Journal of Fluency Disorders, 33, 180–202.
Johnson, W., & Brown, S. F. (1935). Stuttering in relation to vari-
ous speech sounds. Quarterly Journal of Speech, 21, 481–496.

Jones, R. M., Fox, R. A., & Jacewicz, E. (2012). The effects of

concurrent cognitive load on phonological processing in adults
who stutter. Journal of Speech, Language, and Hearing Research,
55, 1862–1875.

Låg, T. (2005). Category-specific effects in object identification:

What is “normal”? Cortex, 41, 833–841.

Laws, K. R., & Neve, C. (1999). A ‘normal’ category-specific
advantage for naming living things. Neuropsychologia, 37,
1263–1269.

This research was funded by the State University of New York
College at Cortland 2010-2011 Faculty Research Program Grant.
The author would like to thank Michaela Granato for assistance
with stimulus selection, participant recruitment, and speech sample
transcription; Mary Emm for assistance with interjudge reliability
measurements; and all individuals who participated in the study.

Levelt, W. J. M. (1989). Speaking: From intention to articulation.

Cambridge, MA: MIT Press.

Lupyan, G., & Mirman, D. (2013). Linking language and categori-

zation: Evidence from aphasia. Cortex, 49, 1187–1194.
Max Planck Institute for Psycholinguistics. (2001). WebCelex.

Retrieved from http://celex.mpi.nl

2548 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 60 (cid:129) 2537–2550 (cid:129) September 2017

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Maxfield, N. D., Huffman, J. L., Frisch, S. A., & Hinckley, J. J.
(2010). Neural correlates of semantic activation spreading on
the path to picture naming in adults who stutter. Clinical Neuro-
physiology, 121, 1447–1463.

Maxfield, N. D., Pizon-Moore, A. A., Frisch, S. A., & Constantine,
J. L. (2012). Exploring semantic and phonological picture-
word priming in adults who stutter using event-related poten-
tials. Clinical Neurophysiology, 123, 1131–1146.

Nespor, M., Peña, M., & Mehler, J. (2003). On the different roles
of vowels and consonants in speech processing and language
acquisition. Lingue e Linguaggio, 2, 203–229.

Osman, A., Lou, L., Muller-Gethmann, H., Rinkenauer, G., Mattes,
S., & Ulrich, R. (2000). Mechanisms of speed-accuracy tradeoff:
Evidence from covert motor processes. Biological Psychology,
51, 173–199.

Paz-Caballero, D., Cuetos, F., & Dobarro, A. (2006). Electrophysi-
ological evidence for a natural/artifactual dissociation. Brain
Research, 1067, 189–200.

Perkins, W. H., Kent, R. D., & Curlee, R. F. (1991). A theory of
neuropsycholinguistic function in stuttering. Journal of Speech
and Hearing Research, 34, 734–752.

Postma, A., & Kolk, H. (1993). The covert repair hypothesis:
Prearticulatory repair processes in normal and stuttered
disfluencies. Journal of Speech and Hearing Research, 36,
472–487.

Proverbio, A. M., Del Zotto, M., & Zani, A. (2007). The emer-

gence of semantic categorization in early visual processing: ERP
indices of animal vs. artifact recognition. BMC Neuroscience,
8, 24.

Psychology Software Tools, Inc. (2012). E-Prime (Version 2.0)

[Computer software]. Retrieved from http://www.pstnet.com
Rastatter, M. P., & Dell, C. W. (1987). Simple versus lexical deci-
sion vocal reaction times of stuttering and normal subjects.
Journal of Fluency Disorders, 12, 63–69.

Riley, G. D. (1994). Stuttering Severity Instrument for Children

and Adults–Third Edition. Austin, TX: Pro-ed.

semantic and phonological encoding during listening and nam-
ing. Neuropsychologia, 40, 778–787.

Sasisekaran, J. (2013). Nonword repetition and nonword reading
abilities in adults who do and do not stutter. Journal of Fluency
Disorders, 38, 275–289.

Sasisekaran, J., & De Nil, L. F. (2006). Phoneme monitoring in

silent naming and perception in adults who stutter. Journal of
Fluency Disorders, 31, 284–302.

Sasisekaran, J., De Nil, L. F., Smyth, R., & Johnson, C. (2006).
Phonological encoding in the silent speech of persons who
stutter. Journal of Fluency Disorders, 31, 1–21.

Sasisekaran, J., & Weisberg, S. (2014). Practice and retention of
nonwords in adults who stutter. Journal of Fluency Disorders,
41, 55–71.

Schmitt, B. M., Schiltz, K., Zaake, W., Kutas, M., & Münte, T. F.
(2001). An electrophysiological analysis of the time course of
conceptual and syntactic encoding during tacit picture naming.
Journal of Cognitive Neuroscience, 13, 510–522.

Sitnikova, T., West, W. C., Kuperberg, G. R., & Holcomb, P. J.
(2006). The neural organization of semantic memory: Electro-
physiological activity suggests feature-based segregation.
Biological Psychology, 71, 326–340.

Smith, A., Sadagopan, N., Walsh, B., & Weber-Fox, C. (2010).

Increasing phonological complexity reveals heightened instabil-
ity in inter-articulatory coordination in adults who stutter.
Journal of Fluency Disorders, 35, 1–18.

van Turennout, M., Hagoort, P., & Brown, C. M. (1998). Brain

activity during speaking: From syntax to phonology in 40 milli-
seconds. Science, 280, 572–574.

Vincent, I., Grela, B. G., & Gilbert, H. R. (2012). Phonological
priming in adults who stutter. Journal of Fluency Disorders,
37, 91–105.

Weber-Fox, C., Spencer, R. M. C., Spruill, J. E., III, & Smith, A.
(2004). Phonologic processing in adults who stutter: Electro-
physiological and behavioral evidence. Journal of Speech,
Language, and Hearing Research, 47, 1244–1258.

Rodriguez-Fornells, A., Schmitt, B. M., Kutas, M., & Münte, T. F.
(2002). Electrophysiological estimates of the time course of

Wijnen, F., & Boers, I. (1994). Phonological priming effects in

stutterers. Journal of Fluency Disorders, 19, 1–20.

Vincent: Semantic and Phonological Encoding in Stuttering 2549

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Appendix

Target labels used for picture stimuli in the semantic and phonological encoding tasks.

Semantic encoding task

Phonological encoding task

Animal subtask

Object subtask

Vowel subtask

Consonant subtask

Bat
Bear
Camel
Cat
Cow
Dolphin
Fish
Fly
Frog
Horse
Leopard
Mouse
Parrot
Penguin
Pig
Pigeon
Rabbit
Shark
Sheep
Snail
Spider
Squirrel
Tiger
Turtle
Zebra

Basket
Boat
Book
Cap
Card
Coat
Drum
Fork
Glasses
Kettle
Lamp
Lighter
Lipstick
Mask
Mitten
Pencil
Plate
Scarf
Scissors
Shirt
Shoe
Spoon
Toaster
Truck
Watch

Acorn
Anchor
Ant
Anvil
Apple
Apron
Arrow
Ax
Eagle
Ear
Egg
Eggplant
Eye
Ice-cream
Igloo
Iron
Oar
Olive
Onion
Orange
Orchid
Ostrich
Otter
Owl
Oyster

Belt
Boot
Bread
Cabbage
Cake
Car
Carrot
Corn
Cup
Glove
Grapes
Kiwi
Lemon
Mushroom
Pan
Pear
Peas
Pepper
Pliers
Pumpkin
Sausage
Skirt
Snowman
Socks
Tie

2550 Journal of Speech, Language, and Hearing Research (cid:129) Vol. 60 (cid:129) 2537–2550 (cid:129) September 2017

Downloaded from: https://pubs.asha.org Michigan State University on 11/10/2025, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions
```