# 1-s2.0-S0094730X21000577-main

```
Contents lists available at ScienceDirect 

Journal of Fluency Disorders 

journal homepage: www.elsevier.com/locate/jfludis 

Adults who stutter do not stutter during private speech 

Eric S. Jackson a,*, Lindsay R. Miller a, Haley J. Warner a, J. Scott Yaruss b 
a Department of Communicative Sciences and Disorders, New York University, 665 Broadway, 9th Floor, New York, NY 10012, United States 
b Department of Communicative Sciences and Disorders, Michigan State University, 1025 Red Cedar Road, East Lansing, MI 48824, United States   

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Stuttering 
Private speech 
Fluency 
Talk-alone-effect 
Social evaluation 
Self-monitoring 

Purpose: Adults who stutter tend not to stutter when they are alone. This phenomenon is difficult 
to study because it is difficult to know whether participants perceive that they are truly alone and 
not being heard or observed. This may explain the presence of stuttering during previous studies 
in which stutterers spoke while they were alone. We addressed this issue by developing a para-
digm that elicited private speech, or overt speech meant only for the speaker. We tested the hy-
pothesis that adults do not stutter during private speech. 
Method:  Twenty-four  participants were  audio-/video-recorded  while  speaking in  several condi-
tions: 1) conversational speech; 2) reading;  3) private speech, in which deception was used to 
increase the probability that participants produced speech intended for only themselves; 4) pri-
vate speech+, for which real-time transcription was used so that participants produced the same 
words  as  in  the  private  speech  condition  but  while  addressing  two  listeners;  and  5)  a  second 
conversational speech condition. 
Results: Stuttering was not observed in more than 10,000 syllables produced during the private 
speech condition, except for seven possible, mild stuttering events exhibited by 3 of 24 partici-
pants. Stuttering frequency was similar for the remaining conditions. 
Conclusions:  Adults  appear  not  to  stutter  during  private  speech,  indicating  that  speakers’  per-
ceptions  of  listeners, whether  real or imagined,  play  a critical  and likely  necessary  role in  the 
manifestation  of  stuttering  events.  Future  work  should  disentangle  whether  this  is  due  to  the 
removal  of  concerns  about  social  evaluation  or  judgment,  self-monitoring,  or  other  communi-
cative processes.   

1. Introduction 

Anecdotal reports indicate that people who stutter tend not to stutter when they are alone, a phenomenon that has been labeled the 
“talk-alone-effect” (Alm, 2014, p. 16). Despite these reports, empirical investigations reveal that people who stutter do actually stutter 
while talking alone, albeit to a significantly reduced degree compared to non-alone conditions (Andrews, Howie, Dozsa, & Guitar, 
ˇ
Sv´ab, Gross, 
1982; Bergmann, 1987; Hahn, 1940; Martin & Haroldson, 1988; Porter & von, 1939; Quinn, 1971; Steer & Johnson, 1936; 
& L´angov´a, 1972). While the impacts of various types of communicative pressure (e.g., presence of audience, communicative intent, 
time pressure) have been studied in speakers who stutter (Bauerly, Jones, & Miller, 2019; Eisenson & Horowitz, 1945; Jackson, Tiede, 
Beal, & Whalen, 2016; Johnson & Rosen, 1937; Sheehan, Hadley, & Gould, 1967), studying speech while talking alone is somewhat 
unique. One reason seems to be that “talking alone” is difficult to operationalize. For instance, if participants surmise that researchers 

* Corresponding author. 

E-mail address: ej34@nyu.edu (E.S. Jackson).  

https://doi.org/10.1016/j.jfludis.2021.105878 
Received 5 April 2021; Received in revised form 31 August 2021; Accepted 31 August 2021   

JournalofFluencyDisorders70(2021)105878Availableonline9September20210094-730X/©2021ElsevierInc.Allrightsreserved.E.S. Jackson et al.                                                                                                                                                                                                     

are listening, are participants truly alone? In the present study, we relied on the phenomenon of private speech – overt speech intended 
only for the speaker – to re-examine the talk-alone-effect. We tested the hypothesis that people who stutter do not stutter during private 
speech, primarily because they do not perceive the presence of a listener. 

1.1. Empirical investigations of the talk-alone effect 

Previous  talk-alone  studies  examined  the  speech  of  people  who  stutter  while  they  were  physically  alone.  Table  1  provides  a 
summary of talk-alone studies. In total, 11 studies since 1936 have included a talk-alone condition. While talk-alone studies consis-
tently reveal a reduction in percent syllables stuttered (%SS), the mean %SS during talk-alone conditions ranged from 2.4–32%. This 
variation between studies may have been impacted by differences in the tasks that were used: reading tasks (Bergmann, 1987; Hahn, 
ˇ
Sv´ab, Gross, & 
1940; Kalinowski, Stuart, Wamsley, &  Rastatter, 1999; Porter &  von, 1939; Quinn, 1971; Steer &  Johnson, 1936; 
ˇ
L´angov´a, 1972) yielded less stuttering than both picture description tasks (Langov´a &  ˇ
Sv´ab et al., 1972a, 1972b) and 
spontaneous speech (Andrews et al., 1982). Researchers’  motivations in these studies appear to have been varied; some looked at 
talking while alone in the context of audience size and audience makeup (Hahn, 1940; Porter & von, 1939; Steer & Johnson, 1936), 
some investigated the impact of “social complexity” (Hahn, 1940) or “communicative situation” (Bergmann, 1987) on stuttering, and 
ˇ
Sv´ab et al., 1972b). 
others directly tested the hypothesis that people who stutter do not stutter when they are alone (Quinn, 1971; 
An important, unanswered question is whether participants in previous talk-alone studies thought that they were truly alone, 
meaning that their speech would not be perceived by a listener, whether during the task or afterwards via audio or video recording. 
Some investigators went great lengths to convince participants (by explanation or deception) to think that they were truly alone, for 
ˇ
Sv´ab et al., 1972b), by standing behind a two-way mirror 
instance, by introducing false recording equipment with deceptive visuals (
(Hahn, 1940), and by leaving participants alone in a car in which they did not expect to be recorded (Quinn, 1971). In the largest study 
of the talk-alone effect, Hahn (1940) implemented deceptive procedures presumably to remove communicative elements entirely. 
During the talk-alone condition in that study, the examiner left the room after instructing participants to inform him when they were 
finished with a task. This was meant to deceive participants into thinking that they were not being observed because apparently the 
examiner would not know when they finished the task. If participants asked about the two-way mirror in the testing room, they were 
told that it was part of the ventilation system. These methodological details suggest that, in addition to leaving participants physically 

Sv´ab, 1973; 

Table 1 
A summary of prior talk-alone studies, including descriptions of experimental tasks, sample sizes, and mean %syllables stuttered.  

Study 

Talk-alone task(s) 

Steer & 

Johnson, 
1936 
Porter & von, 
1939 
Hahn, 1940 
Quinn, 1971 

ˇ
Sv´ab et al., 
1972a, 
1972b 

Langov´a & 
ˇ
Sv´ab, 1973 

Hood, 1974 

Andrews et al., 
1982 

Bergmann, 

1987 
Martin & 

Haroldson, 
1988 

Kalinowski 

et al., 1999 

Reading alone while timing oneself; reading alone with 
surreptitious recording equipment 

Reading with an experimenter listening in the next room; 
speakers were asked to mark moments of stuttering 
Reading with a concealed experimenter 
Reading with surreptitious recording equipment; speakers 
asked to mark moments of stuttering 
Reading and picture description in 
three “alone” conditions: 
One with surreptitious recording, one with known recording, 
one with live monitoring outside of the room 
(As above) reading and picture description in three “alone” 
conditions: one with surreptitious recording, one with known 
recording, one with live monitoring outside of the room 
Picture description in three alone conditions: one with only 
surreptitious recording, one in which the participant was aware 
of visual video recording without audio, and one in which the 
participant was aware of audio recording without video 
Alone condition 1: Speaking on any topic; experimenter left the 
room 
Alone condition 2: Speaking on a provided (card) topic; 
experimenter left the room 
Reading without an experimenter present 

Talking extemporaneously before and after a conversation with 
the experimenter. During conversation, experimenters 
commented on what speakers had said. 
Alone condition 1: reading alone with surreptitious recording 
Alone condition 2: reading alone with known audiovisual 
recording  

Sample 
size 

n = 10 

n=13 

n=52 
n=11 

n = 23 

n = 10 

n=20 

n=3 

n=5 

N = 10 

Mean % syllables stuttered in alone condition 

% syllables stuttered not reported; AWS produced fewest 
instances of stuttering in these two alone conditions (with the 
least in the surreptitious recording condition) 
2.8% of words stuttered 

4.5% syllables stuttered 
3.6 % syllables stuttered 

Approximately 6(cid:0) 7% syllables stuttered during reading. 
Approximately 10(cid:0) 13% syllables stuttered during picture 
description 

3% syllables stuttered during reading. 
24 % syllables stuttered during picture description 

12.7% in no monitoring condition, 13.8 % in video-only 
recording condition, 15.3 % in audio recording condition 

2.4% syllables stuttered with no topic cards 
20.3 % syllables stuttered with topic cards 

4.4% syllables stuttered; 1.1 % when the first participant was 
excluded from data 
14.4% syllables stuttered pre-conversation, 15.1 % syllables 
stuttered post-conversation 

Approximately 32 % syllables stuttered without known 
recording; 
Approximately 28 % syllables stuttered with known 
recording 

*Frequency altered feedback (FAF) was used in some alone conditions during this study. The reported results reflect experimental conditions that did 
not use FAF. 

JournalofFluencyDisorders70(2021)1058782E.S. Jackson et al.                                                                                                                                                                                                     

alone, experimenters were attempting to reduce any suspicion that they were monitoring or analyzing participants’ speech during the 
talk-alone conditions. Even the potential to be heard introduces a possible listener or audience, and this, in turn may increase concern 
about listener approval or social evaluation, communicative intent, propositionality or meaning, or time pressure, all of which increase 
the frequency of overt stuttering events (e.g., Byrd, Coalson, & Bush, 2010; Commodore & Cooper, 1978; Eisenson & Horowitz, 1945; 
Porter & von, 1939; Ramig, Krieger, & Adams, 1982; Sheehan et al., 1967; Steer & Johnson, 1936; Yovetich & Dolgoy, 2001). It is not 
necessary for a listener to be physically present in order for the speaker to perceive that their speech will be heard. 

Despite the deception components in talk-alone studies, cues within research settings may have caused participants to suspect that 
ˇ
Sv´ab et al. (1972a) and Andrews et al. (1982), the recording equipment in the testing room 
they were being monitored. For instance, in 
was visible to participants. In Hood (1974), the experimenter wore a mask and headphones so that they could not hear participants 
during the alone condition, but they were still present in the room. Further, the goal of the alone tasks in many of these studies was to 
produce audible speech. The methodology of Hahn (1940) is indicative of the complexity of this issue. Participants were instructed to 
read  aloud  and  mark  their  own  stuttering  events.  While  this  may  have  furthered  the  deception  that  participants  were  not  being 
observed, marking one’s own stuttering events emphasized that speech was the goal of the task. This may have caused participants to 
serve as their own “critical listeners” self-evaluating their speech. The suggestion that someone is being listened to or is imagining how 
they would sound to a listener has been proposed to be a sufficient condition to trigger stuttering (Alm, 2014; Bluemel, 1935, as in 
Bloodstein & Bernstein-Ratner, 2008). Despite the fact that participants were physically alone and despite the methodological steps 
taken by Hahn (1940), speakers may have perceived that their speech would be heard or that their message would be interpreted 
because of the experimental setting, instructions to speak aloud, or instructions to self-monitor stuttering. Experimental studies of the 
talk-alone effect, then, need to go beyond convincing participants that they are physically alone to create conditions in which par-
ticipants do not perceive that their speech will be heard, or at least that participants will not be thinking about their speech being 
heard. Here, we relied on the phenomenon of private speech to create such conditions. 

1.2. Private speech 

“Private speech” has been defined, broadly, as audible speech that is not intended for or directed to others (Piaget, 2002; Vygotsky, 
1934). In other words, private speech occurs when people “talk to themselves” out loud, without prompting. According to Vygotsky 
(1934), children use private speech as a means to self-regulate cognition and behavior, for example, when completing a difficult task. 
Throughout development, private speech transitions to inner speech, such that the child is no longer using their speech mechanism 
(respiration,  phonation,  articulation)  but  continues  to  self-regulate  (Alderson-Day  &  Fernyhough,  2015;  Winsler,  Fernyhough,  & 
Montero, 2009), perhaps because talking to oneself is no longer considered socially appropriate during the school-age years. Empirical 
research since Vygotsky indicates that private speech is actually used for numerous functions (e.g., practice for social encounters, 
language practice), and that private speech continues into adulthood (Alderson-Day & Fernyhough, 2015; Berk, 1999). 

In contrast with talking alone, private speech is not defined by merely the physical absence of a listener. Rather, private speech is 
defined by the absence of the speaker’s perception that their speech will be perceived by a listener, because 1) there is no possibility of 
a listener perceiving the message (e.g., the speaker is in their bedroom and nobody else is home) or 2) the speaker’s attention is not 
directed towards the possibility of their speech being heard (e.g., they are not thinking about it). During private speech, there is no 
concern or awareness by the speaker that their message will be perceived by an observer. It is possible to engage in private speech when 
in  the  presence  of  others,  as  when  speaking  aloud while  trying  to  remember  a  grocery  list  in  public  or  following  instructions  to 
assemble a piece of furniture. In these cases, there are potential listeners present, but the speaker does not intend to communicate to 
these listeners and is not attending to the possibility of their message being heard by these listeners. Conversely, being alone does not 
ensure that someone is engaging in private speech. For instance, a speaker may be aware that a listener, not in the room, is in earshot, 
or that they may be heard by somebody in the future (i.e., if they are recorded). Private speech can occur spontaneously or during 
reading, as long as the speaker is not perceiving a (potential) listener. 

Most studies on private speech have focused on young children who are thought to engage in private speech for language practice 
(Kohlberg, Yaeger, & Hjertholm, 1968; Krafft & Berk, 1998; Manning, White, & Daugherty, 1994) and self-regulation (Berk, 1999). 
Although studies that have examined private speech in adults are more limited, they offer insights into the types of tasks that elicit 
private speech in this age group. In a study of spontaneous speech taken from a continuous recording of two participants throughout 
the day, Soskin and John (1963) found that private speech was used when adult participants were learning a new manual task (i.e., 
learning to craft lanyards and moccasins). Duncan and Cheyne (2001) found that the difficulty, modality, and familiarity of experi-
mental tasks all play a role in the frequency of adult private speech. During two sessions, participants in their study performed one easy 
and one difficult computer task, as well as a challenging paper-folding task. Participants exhibited the greatest amount of private 
speech during the difficult computer task, which required them to enter a complex data set into a spreadsheet given complicated 
instructions (Duncan & Cheyne, 2001). Taken together, these studies suggest that complex tasks, such as computer programming, elicit 
more private speech than simple ones. 

The present study relied on the phenomenon of private speech to examine the widely known but poorly understood talk-alone- 
effect. By  focusing on  this type of  speech  meant  only  for the  self, we created  a  condition in  which  it was  unlikely  that speakers 
would perceive that someone was listening to them. Various techniques were implemented to convince participants that their speech 
would not be heard, and to redirect participants’ focus from a possible observer (i.e., video recording), including deception and use of 
an experimental task that did not require speaking aloud. These methods increased the probability that participants would feel that 
they were truly alone and engage in speech that was intended solely for themselves. In addition, real-time transcription using multiple 
transcribers allowed for a private speech+ condition, in which participants produced the same words as in the private speech condition, 

JournalofFluencyDisorders70(2021)1058783E.S. Jackson et al.                                                                                                                                                                                                     

but in the presence of two listeners. Two conversational speaking tasks and a reading task were also used for comparison. We hy-
pothesized that stuttering would be absent during private speech and present at comparable levels during all other conditions (private 
speech+, spontaneous conversation, reading). 

2. Methods 

2.1. Participants 

This study, including deception procedures, was approved by the Institutional Review Board at New York University. All subjects 
provided informed consent to participate. Twenty-three adults who stutter were included, ranging in age from 18 to 50 years (7 female; 
M = 31.17, SD = 8.65). The female-male ratio is in line with current prevalence data on stuttering (Yairi & Ambrose, 2005). Par-
ticipants were self-reported native speakers of Standard American English (i.e., they learned English before age six and speak it daily) 
who self-reported a history of stuttering and negative histories of other speech, language, cognitive, or neurological impairments. All 
participants were screened informally for hearing and visual impairment during conversational interactions with the examiners. All 
participants were judged by the first author, a licensed speech-language pathologist with more than 10 years of expertise in stuttering 
intervention, to exhibit at least 3 stuttering-like disfluencies (SLDs; Yairi & Ambrose, 1992) with concomitant accessory behaviors (e. 
g.,  eye  blinking,  head  tilting)  during  a  5–10  min.  conversation.  In  addition,  all  participants  scored  1.56  or  above  on  the  Overall 
Assessment of the Speaker’s Experience of Stuttering (OASES) (Yaruss & Quesal, 2016) and 8 or above on the Stuttering Severity Instrument - 
4th Edition (SSI-4) (Riley, 2009). Participant characteristics and test scores are included in Table 2. These inclusionary criteria provided 
a comprehensive assessment of each participant’s stuttering. Non-standardized SSI-4 scores were calculated for four participants who 
did not complete the reading sample; for these participants, 2 conversational samples (as opposed to 1 reading, 1 conversational) were 
used for SSI-4 scores. 

2.2. Procedure 

Speech samples were collected for 5 conditions (see below). Given that the amount of private speech used by individuals was 
expected to be variable and perhaps limited, 150 syllables was determined to be the minimum number of syllables to be collected 
during each condition. One hundred fifty syllables is also the minimum number of syllables per sample required for the SSI-4. It was 
first necessary to develop a task that would elicit enough private speech for analysis. Based on findings from Duncan and Cheyne 
(2001)  that  difficult  and  novel  computer-based  tasks  elicit  more  private  speech  than  easier,  non-computer  tasks,  we  designed 
computer-based tasks for this experiment. The tasks utilized Scratch (MIT Media Lab), a program geared towards teaching computer 
coding to children in which users create animation, games, and other interactive content. While Scratch tasks can be challenging, they 
are specifically designed so that children can complete them. Participants code their content using graphical programming blocks and 
other visual tools, such as a paintbrush and eraser. Written instructions for participants were refined during piloting, and the verbal 

Table 2 
Participant  demographic  and  stuttering  severity  characteristics.  Age  in  years.  OASES  = Overall  Assessment  of  the  Speaker’s  Experience  of 
Stuttering; SSI-4 = Stuttering Severity Instrument—Fourth Edition.  

Participant 

P01 
P02 
P03 
P04 
P05 
P06 
P07 
P08 
P09 
P10 
P11 
P12 
P13 
P14 
P15 
P16 
P17 
P18 
P19 
P20 
P21 
P22 
P23 
mean (SD) 

Age 

29 
36 
40 
49 
50 
23 
26 
46 
37 
32 
25 
25 
28 
31 
33 
35 
28 
30 
20 
28 
22 
25 
19 
31.17 

Gender 

OASES 

M 
M 
M 
F 
F 
M 
M 
M 
F 
M 
F 
F 
M 
M 
F 
M 
M 
M 
M 
M 
M 
F 
M 
– 

2.48 
3.09 
3.33 
2.58 
2.35 
1.56 
3.26 
2.5 
3.41 
2.73 
3.08 
3.09 
2.48 
2.06 
2.15 
3 
3.58 
3.01 
3.2 
2.39 
3.73 
2.57 
2.53 
2.79 

*indicates non-standardized scores (see text for details). Non-standardized scores were not included in mean calculations. 

SSI-4 

14* 
21* 
32* 
17* 
15 
8 
28 
18 
25 
27 
15 
26 
20 
22 
23 
30 
11 
11 
34 
28 
27 
8 
23 
21.00 

JournalofFluencyDisorders70(2021)1058784E.S. Jackson et al.                                                                                                                                                                                                     

instructions  were  adjusted  to  deceive  participants  into  thinking  that  any  private  speech  was  only  for  their  own  benefit  during 
completion of the computer tasks. It was expected that participants would have varying degrees of familiarity with coding tasks. 
Therefore, the written instructions did not assume prior Scratch or coding experience. 

The experiment was completed during a single session lasting up to 90 min. Sessions were recorded on two cameras. One camera 
captured a clear view of the participant’s upper body and face when they were seated, in order to maximize the investigators’ view of 
any accessory behaviors such as eye blinking and head movements. Another camera captured a view of the computer screens, which 
facilitated real-time speech transcription by allowing the transcribers to see the task instructions that participants saw. This was helpful 
when speech was produced at a low volume. A single “room” microphone placed by the first camera was used to record audio. Video 
feeds were viewed by investigators in order to conduct real-time transcription (see private speech+ condition) and were recorded for 
post hoc review and transcription of all conditions. Five conditions were included in the study in order to collect a variety of speech 
samples in different contexts. 

Condition 1: Spontaneous Speech Sample 
Two investigators initiated a conversation with the participant. Investigators used informal conversational questions and state-
ments (e.g., “Tell us a little bit about yourself.”) to prompt discussion. Investigators transitioned to the second condition once they 
estimated that the participants had produced at least 150 syllables. The first 150 syllables uttered by the participant, as determined by 
transcription following the experiment, were used for analysis. 

Condition 2: Reading Sample 
Participants read aloud the 230-syllable adult-level reading sample from the SSI-4 (Plate XI). Both investigators remained in the 

room during this condition. 

Condition 3: Private Speech 
The purpose of this condition was to elicit and record participants’ private speech as they completed difficult computer-based tasks. 
Participants were left alone in the room to complete three timed tasks using an offline desktop version of Scratch on a laptop computer. 
The first task was to program a cat-and-dog-themed game, the second task was to program a racecar-themed game, and the third task 
was to recreate the cat-and-dog-themed game from memory. Written directions to create the two games were provided on a second 
laptop. These included descriptions of how to create elements of the game and screenshots showing what the participants’ Scratch 
windows should look like at different points in the process. In general, participants shifted gaze between the instruction monitor and 
the Scratch monitor. Because the instructions were presented on the monitor, some of the private speech in this condition appeared to 
be read. No written directions were provided for the third task. 

Deception was used to increase the probability that participants would use private speech while they carried out the three tasks. 

The following script was read before the first task was administered: 

There’s research from psychology that when people talk to themselves out loud, they perform better. We strongly encourage talking out 
loud during the task. Because we’re in a clinic room, there are cameras here, but they are not on. We might ask you after the task if, and 
how much, you were talking. 

The decision to address the presence of cameras was based on reports from several pilot participants that they had noticed the 
cameras and surmised that researchers were observing them. During each task, investigators remained in a separate room except to 
give a ten-minute warning. Following the first task, participants were asked if they had spoken out loud and approximately how much, 
in  order  to  perpetuate  the  deception  that  they  were  not  being  observed.  Participants  were  also  given  the  reminder,  “again,  we 
recommend that you talk out loud.” 

Three investigators, different from those in Condition 1, remained in a separate observation room to transcribe the participants’ 
utterances in real time onto a Google Slides presentation. Using three transcribers was intended to maximize accuracy of the tran-
scriptions and to count the total number of syllables in real time. All transcribers had successfully completed graduate level tran-
scription analysis training through coursework. The private speech condition was concluded once the time limit expired for all tasks 
(30 min) or once researchers estimated that they had transcribed more than 150 syllables uttered by the participant. A full tran-
scription of speech uttered during this condition was completed following the experiment to ensure accuracy. 

Prior to Condition 4, participants were informed of the deception component of the study—that they were in fact being observed 
and recorded during the private speech condition. It was explained to participants that the deception component was the only way to 
create a condition in which private speech would be elicited, and recorded. Participants were then asked if they consent to continuing 
with the experiment, and were informed that if they were not, all of their data would be destroyed immediately. All participants 
consented to continue with the study. One participant complained about the deception component, but after further explanation 
regarding the purpose of the study, this concern was resolved. 

Condition 4: Private Speech+
This condition was designed to elicit the same linguistic content that the participants uttered during the private speech condition, 
but with two investigators present in the testing room. Participants produced their own utterances from the private speech condition 
while viewing them on Google Slides. One utterance was presented per slide. Because of concern that a rhythm effect would potentially 
enhance fluency, slides were advanced by the examiner using a concealed clicker in jittered time increments of approximately 3–6 s. 
between slides. This reduced participants’ ability to predict when the next slide would appear. Participants were instructed to maintain 
eye contact with one of the examiners while speaking. Effectively, participants were required to hold the text in working memory, 
which has been shown to both increase and reduce stuttering (Bosshardt, 1999; Eichorn, Marton, Schwartz, Melara, &  Pirutinsky, 
2016; Oomen & Postma, 2002). Similar to the private speech condition, there were instances during which participants read from the 
monitor. Given the experimental layout, it was difficult to quantify these amounts and therefore they are not included in the results. 

JournalofFluencyDisorders70(2021)1058785E.S. Jackson et al.                                                                                                                                                                                                     

However, based on informal observation, there were approximately equal amounts of spontaneous speech and reading during the 
private speech and private speech+ conditions. 
Condition 5: Spontaneous Speech Sample 
Participants engaged in conversation with the investigators, similar to Condition 1. This included asking participants to reflect on 
the procedures and methods of the study, including the deception inherent to the study design. A variety of other questions were used 
as conversation prompts. For instance, participants were asked about their familiarity with coding tasks or about their use of private 
speech in everyday situations. Investigators concluded the experiment once they estimated that the participant had produced at least 
150 syllables. As with condition 1, after transcription of this condition post hoc, the first 150 syllables uttered by the participant were 
used for analysis. 

2.3. Data analysis 

2.3.1. Percent syllables stuttered (%SS) 

%SS was calculated for each of the 5 conditions. Total syllable counts for each speech sample are presented in Table 3. Two Master’s 
degree students in Communicative Sciences and Disorders, trained in stuttering frequency analysis through graduate-level coursework, 
identified SLDs, including blocks, prolongations, and part-word repetitions, as well as associated behaviors, such as eye-blinking and 
visible muscle tension. Other disfluencies, including polysyllabic whole word repetitions and fillers (e.g., “um”,  “like”), were not 
counted as SLDs. Discrepancies were reconciled through unanimous consensus. Unanimous consensus was not reached for 23 potential 
instances of stuttering for 11 of the participants, requiring involvement of the first author, who made the final judgment. In addition, 
the final author who was blind to all stuttering judgments, judged these 23 instances, and there was perfect agreement between the 
fourth and fifth authors. The final, reconciled transcripts were used to calculate %SS for all participants across the five conditions. 
To test whether there was more stuttering in the private speech+ than the private speech condition, and whether there were 
significant differences between private speech+ and the conversational speech and reading conditions, a linear mixed-effects model 
was fit using the lme4 package (Bates, M¨achler, Bolker, & Walker, 2015) and lmerTest package (Kuznetsova, Brockhoff, & Christensen, 
2017) in R (R Core Team, 2020). Condition was included as a fixed factor, and participant was included as a random factor to account 
for expected variation due to individual differences. All conditions were compared to the private speech+ condition so that differences 
between private speech and private speech+, as well as between private speech+ and the remaining conditions, could be determined. 
A paired t-test was also used to examine potential order effects, specifically between the two conversation conditions (conditions 1 and 
5). 

Whispered and unintelligible speech during the private speech condition were not included in any measures. Whispered speech, 
which was identified by absent voicing, has been shown to reduce stuttering (Commodore &  Cooper, 1978; Ingham et al., 2009; 
Perkins, Rudas, Johnson, & Bell, 1976). Percent of whispered syllables was calculated for each participant ([total number of whispered 

Table 3 
Syllable counts and percent syllables stuttered (%SS) for each condition. Variations in the total number of syllables in the reading condition reflects 
participants’ pronunciations. The private speech syllable counts exclude any syllables that were unintelligible or whispered. C1 and C2 = Conver-
sation 1 or 2.  

Participant 

C1 

Reading 

Private Speech 

Private Speech+

C2  

P01 
P02 
P03 
P04 
P05 
P06 
P07 
P08 
P09 
P10 
P11 
P12 
P13 
P14 
P15 
P16 
P17 
P18 
P19 
P20 
P21 
P22 
P23 
mean 

Syllables 

%SS 

Syllables 

%SS 

Syllables 

N/A 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 

N/A 
8.67 
12.67 
2.67 
2.67 
3.33 
4 
3.33 
5.33 
10.67 
3.33 
10 
9.33 
14 
8 
12 
1.33 
0.67 
12.67 
6 
18.67 
0.67 
14 
7.46 

N/A 
N/A 
N/A 
N/A 
227 
228 
228 
227 
227 
226 
228 
227 
227 
226 
228 
227 
227 
227 
228 
227 
228 
226 
226 
227.11 

N/A 
N/A 
N/A 
N/A 
2.63 
0 
3.95 
.45 
10.52 
24.67 
4.82 
10.09 
1.30 
4.35 
10.09 
1.30 
4.35 
0.89 
31.14 
10.53 
1.75 
0.44 
7.56 
6.89 

130* 
526 
988 
407 
365 
632 
226 
1397 
338 
185 
347 
16* 
536 
332 
22* 
591 
123* 
722 
746 
37* 
443 
698 
275 
438.35 

%SS 

0 
0 
0.3 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0.28 
0.27 
0 
0 
0 
0 
0.04 

Syllables 

%SS 

Syllables 

%SS 

160 
246 
480 
223 
220 
407 
189 
534 
250 
139* 
288 
7* 
261 
216 
35* 
321 
127* 
320 
227 
122* 
185 
295 
157 
235.17 

1.25 
8.54 
2.08 
4.48 
1.82 
0.49 
3.70 
0 
0.4 
10.79 
8.68 
71.43 
0 
0.46 
17.14 
5.61 
0 
1.25 
42.29 
8.40 
3.24 
0.34 
1.27 
8.42 

150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 
150 

2.67 
5.33 
12 
3.33 
6.67 
2 
4.67 
0.67 
0.67 
14 
2.67 
9.33 
3.33 
8.66 
7.33 
5.33 
1.33 
0 
11.33 
5.33 
9.33 
2 
10 
5.56 

*indicates a syllable count less than the 150-syllable target for private speech and private speech+ conditions. 

JournalofFluencyDisorders70(2021)1058786E.S. Jackson et al.                                                                                                                                                                                                     

syllables]/[total number of syllables]*100). Percent of unintelligible syllables was also calculated for each participant ([total number 
of unintelligible/total number of syllables]*100). 

2.3.2. Percent of same syllables 

The conditions were designed such that the text in the private speech+ condition matched the text in the private speech condition. 
However, due to limitations of real-time transcription during the private speech condition, some utterances were not transcribed 
accurately. Therefore, the percent of same syllables between the private speech and private speech+ conditions was calculated. For each 
participant, each utterance from the private speech+ condition was compared to each utterance from the private speech condition. To 
determine the percentage of same syllables in both conditions, the total number of syllables from the private speech+ condition that 
were the same in the private speech condition was divided by the total number of private speech+ syllables. ([# matching syllables 
between the conditions]/[total # syllables in the private speech+ condition])*100. 

2.3.3. Speech motor factors 

Given the current design, it was not possible to control for speech rate, amplitude, and prosody between the private speech and 
private speech+ conditions. Therefore, we tested whether these variables changed between the private speech and private+ condi-
tions. Utterances were segmented using CLAN software (MacWhinney, 2000), and then imported into Praat (Boersma &  Weenink, 
2020) for analysis. A random 30 % of utterances were analyzed. Speech rate was calculated as syllables per second. Average amplitude 
(dB) of each utterance was calculated using the Praat loudness function. Prosody was determined by measuring the standard deviation 
(SD) of fundamental frequency (f0) of the utterance. Utterances containing whispered speech and unintelligible speech were excluded. 

3. Results 

3.1. Percent syllables stuttered (%SS) 

Twenty-three  participants produced  a  total of  10,844  syllables during  the  private speech  condition.  A total  of  seven syllables 
produced during private speech were judged to be stuttered. These seven instances were distributed across three participants. Each of 
these instances of stuttering had a duration of less than one second (M = 0.47 s; SD = 0.30 s) and was associated with minimal or no 
observable physical tension (see Table 4). Across participants, 526 syllables were judged to be whispered (~5%) and 235 were judged 
to be unintelligible (~2%) (see Table 5). Within participants, a mean of 22.9 syllables were whispered (SD = 22.0) and 10.2 syllables 
were unintelligible (SD = 7.6). Descriptive statistics for %SS per participant and condition revealed the following: conversation 1 (M =
7.46, SD = 5.13); reading (M = 6.89, SD = 8.34); private speech (M = 0.04; SD = 0.09); private speech+ (M = 8.42, SD = 16.48); 
conversation 2 (M = 5.56, SD = 4.03) (see Table 3). There was significantly less stuttering during private speech compared to private 
̂β = -1.035, 
̂β = 8.38, t = -3.65, p < .001), and there were no differences between private speech+ and conversations 1 and 2 (
speech+ (
̂β = -1.456, t = -0.6, p > .10) (see Fig. 1). A paired t-test 
t = -0.45, p > .10; 
revealed a lower %SS in conversation 2 than conversation 1 (t = 2.37, df = 22, p < .05). Sixteen of 22 participants (72.7 %) exhibited 
less stuttering in conversation 2 compared to conversation 1. 

̂β = -2.855, t = -1.24, p > .10, respectively) and reading (

3.2. Percent of same syllables 

It was necessary to assess utterance similarity between the private speech and private speech+ conditions because not all syllables 
were accurately transcribed in real time. Across all participants, 64.72 % of syllables present during the private speech+ condition 
matched exactly with those in the private speech condition. 

Table 4 
Qualitative descriptions of all moments of stuttering that occurred during the private speech condition, including the type of disfluency, associated 
behaviors, and durations.  

Participant 

P03 

P18 

P19 

Stuttered 
word 

Stage 
Select 

The 

Create 
Showed 

Whatever 
Fifty 

Qualitative Description 

Duration of 
stuttering 

Prolongation of the initial /s/ phoneme. No associated tension or accessory behaviors. 
Part-word repetition, single iteration of initial syllable /sə/. No associated tension or accessory behaviors. 
Part-word repetition, single iteration of /t/ produced prior to production of “the.” No associated tension or 
accessory behaviors. 
Block (i.e., “c_create”). No associated tension or accessory behaviors. 
Part-word repetition, single iteration of /ʃ/, no voicing. No associated tension or accessory behaviors. 
Block on /ε/ phoneme at the initiation of second syllable. Mild tension of the jaw. Minimal eye blinking noted 
before initiation of word. 
Block on /f/ followed by prolongation. Dysrhythmic phonation noted during initiation of voicing. Mild 
tension of the lips while posturing for /f/. Minimal eye blinking noted during block. 

0.67 
0.22 

0.27 

0.73 
0.13 

0.93 
0.37  

JournalofFluencyDisorders70(2021)1058787E.S. Jackson et al.                                                                                                                                                                                                     

Table 5 
Amount of syllables, by participant, that were whispered or unintelligible during the private speech condition.  

Participant 

Total private speech 
syllables 

Number of whispered 
syllables 

Number of unintelligible 
syllables 

Percent 
whispered 

Percent 
unintelligible 

P01 
P02 
P03 
P04 
P05 
P06 
P07 
P08 
P09 
P10 
P11 
P12 
P13 
P14 
P15 
P16 
P17 
P18 
P19 
P20 
P21 
P22 
P23 
mean 

164 
555 
1000 
442 
420 
666 
306 
1402 
353 
191 
403 
80 
557 
383 
61 
591 
145 
743 
828 
74 
461 
709 
309 
471.43 

31 
2 
0 
8 
49 
24 
73 
0 
3 
6 
51 
52 
14 
45 
28 
0 
22 
11 
59 
23 
5 
1 
19 
22.87 

3 
27 
12 
27 
6 
10 
7 
5 
12 
0 
5 
12 
7 
6 
11 
0 
0 
10 
23 
14 
13 
10 
15 
10.22 

18.90 
0.36 
0.00 
1.81 
11.67 
3.60 
23.86 
0.00 
0.85 
3.14 
12.66 
65.00 
2.51 
11.75 
45.90 
0.00 
15.17 
1.48 
7.12 
31.08 
1.08 
0.14 
6.15 
11.49 

1.83 
4.86 
1.20 
6.11 
1.43 
1.50 
2.29 
0.36 
3.40 
0.00 
1.24 
15.00 
1.26 
1.57 
18.03 
0.00 
0.00 
1.35 
2.78 
18.92 
2.82 
1.41 
4.85 
4.01  

Fig. 1. Percent syllables stuttered (%SS) for each condition, including quantile whiskers. Dots indicate %SS for each participant. Black lines indicate 
mean values. 

3.3. Speech motor factors 

Mean speech rate was 3.17 syllables per second during private speech and 3.46 syllables per second during private speech+. A 
significant difference was not observed for speech rate when comparing the two conditions (t = -1.15, p > .1). Mean amplitude was 

JournalofFluencyDisorders70(2021)1058788E.S. Jackson et al.                                                                                                                                                                                                     

significantly lower during private speech (mean = 63.22 dB) than private speech+ (mean = 70.3 dB) (t = -19.17, p < .001). Mean f0 
was 105.60 Hz during private speech and 117.03 Hz during private speech+. Prosody was measured as the SD of f0. SD of f0 was 21.28 
Hz during private speech and 30.28 Hz during private speech+. SD of f0 was significantly lower in the private speech versus private 
speech+ condition (t = -2.72, p < .05). 

4. Discussion 

This study relied on the phenomenon of private speech to examine the well- known but poorly understood talk-alone effect. The 
elimination of stuttering while speakers talk alone is often reported anecdotally. Prior to the current study, these reports had not been 
confirmed empirically. While overt stuttering was significantly reduced in previous talk-alone studies, there remained a considerable 
amount of stuttering during talk-alone conditions in many of those studies. We sought to reconcile this discrepancy based on the 
premise that speakers in previous studies may have perceived that their speech would be heard by a listener, either in the moment or 
after via audio or video recordings. To  this end, we carefully designed a condition in which the probability that speakers would 
perceive a listener was significantly reduced (i.e., the private speech condition). We compared the level of stuttering in this condition 
to a condition in which participants produced the same utterances as in the private speech condition, but with listeners present (i.e., 
private speech+ condition), as well as reading and spontaneous speech conditions. 

4.1. Stutterers do not stutter during private speech 

Three of 23 participants produced a total of 7 stuttered syllables in more than 10,000 syllables of private speech. Importantly, our 
method for judging SLDs was conservative, and some of these instances might not be considered SLDs by other researchers or SLPs (e. 
g., part-word repetitions with a single iteration and no associated tension). This suggests that the actual number of stuttering events 
across all participants in the private speech condition may have been even less than seven. It is possible that during these seven in-
stances of possible stuttering, participants suspected that their speech would be heard, despite our careful efforts to prevent such cases. 
It is also possible that participants may have been thinking about how they might sound if they were being observed, or they might 
have thought that their speech was actually being recorded by the cameras in the room. Moreover, the participants knew that they 
were invited to participate in this study because they stutter, and they knew that stuttering was a topic of interest to the researchers. 
Thus, it is possible that some or all of these possible stuttering events were due to the speaker perceiving that they were being observed. 
This highlights a challenge to private speech studies in general: the speaker’s perception of speech not being heard is determined by 
their state of mind in a given moment. Although the current study was designed carefully to elicit private speech, we cannot know with 
certainty that participants were engaging in private speech in all cases. Still, the extremely low incidence of stuttering during private 
speech provides support for our hypothesis that speakers who stutter do not stutter if they do not perceive that someone is listening to 
them. 

The presence of a listener, or merely thinking about a listener, introduces the possibility that the speaker will be evaluated socially. 
When engaging in private speech, a speaker does not perceive a listener, either because the speaker thinks that there is no possibility of 
someone perceiving their speech, or because the speaker’s attention is not directed towards a potential listener. When this is the case, 
the speaker is not thinking about how they are being perceived by a listener or how the listener may be judging them—there is no social 
evaluation. In addition, the perception or awareness of a listener could also introduce communicative intent, commonly defined as 
communication acts aimed to affect the behavior or attitudes of others (Prizant & Wetherby, 1987). A speaker’s intent to communicate 
may be present any time they intend to send a message, intend their message to be heard, or even perceive that the content that they 
generate will be perceived by another. By contrast, private speech only involves affecting one’s own perceptions and behaviors and 
therefore lacks communicative intent. 

In previous talk-alone studies, participants may have thought that they were being observed because of cues in the research setting, 
ˇ
Sv´ab et al., 1972a) or a two-way mirror (e.g., Hahn, 1940). 
such as recording equipment (e.g., Andrews et al., 1982; Hood, 1974; 
Despite how discreet investigators’ observations may have been in these early studies, the cues or requirements to produce speech may 
have caused participants to believe that investigators were monitoring their speech, either in-the-moment or after-the-fact via audio or 
video recordings. In contrast to prior studies, speech was not required during the computer-based private speech tasks of the present 
study. Therefore, the current study extends the talk-alone paradigm by reducing or entirely removing the speaker’s perception that 
their speech would be heard. 

4.2. Private speech and self-monitoring 

It may be the case that during private speech, the speaker is simply not attending to or monitoring the possibility of a listener, 
regardless of whether that possibility exists. This might be the case when someone in a supermarket aisle talks to themselves about 
what they need or reads aloud a list from their phone. There may be potential listeners present, but the speaker is not attending to or 
focusing on those listeners, and the speaker is not intending to communicate with those listeners. 

One possible (but less likely) explanation for this effect is the element of distraction, a factor that has been something of an enigma in 
prior stuttering research. The elimination of stuttering during private speech may be due to distraction from a potential listener and all 
that that entails (e.g., social judgment, communicative load, anxiety). Numerous studies on displacement of attention, or distraction, 
have shown that factors such as novel modes of speaking (e.g., Johnson & Rosen, 1937; Kalinowski et al., 1999), associated activity (e. 
g., Arends, Povel, & Kolk, 1988; Eichorn et al., 2016), or intense or unusual stimuli (e.g., Brin, Stewart, Blitzer, & Diamond, 1994; 

JournalofFluencyDisorders70(2021)1058789E.S. Jackson et al.                                                                                                                                                                                                     

Sayles, 1971) may reduce the frequency of stuttering. One psycholinguistic account that is in line with distraction hypotheses is the 
vicious circle hypothesis (VCH), which posits that stuttering results from overactive self-monitoring of temporal aspects of speech 
(Vasic & Wijnen, 2005). According to the VCH, monitoring will be reduced when cognitive resources are devoted to something other 
than speech, leading to reduced stuttering (Vasic & Wijnen, 2005). The computer tasks during the private speech condition may have 
prevented participants from “over-monitoring” their speech. 

Ultimately, however, studies on distraction have not clarified the talk-alone-effect, mainly because it is difficult to operationalize 
the distraction phenomenon (Bloodstein & Bernstein-Ratner, 2008, p. 271). For example, questions remain about what the speaker is 
being distracted from: is it their speech mechanism, the anticipation of stuttering, or more generally, stuttering itself? The detrimental 
effects of “over-focusing” on the speech system are supported by studies that have shown that diverting attention internally actually 
impedes general motor performance (e.g., Kal, Van der Kamp, & Houdijk, 2013; McNevin, Shea, & Wulf, 2003). It is plausible that 
increased internal focus results from the presence of a potential listener, perhaps due to the possibility of social evaluation which may 
result  in  increased  self-evaluation.  For  example,  Buhr,  Scofield,  Eyer,  and  Walden  (2017)  found  that  increased  disfluencies  in 
typically-developing speakers resulted from increased self-monitoring due to a live video feed of one’s own behavior during a speaking 
task. A focus on the speech system, anticipation, or stuttering more generally, would all emanate from attention to a (possible) listener. 
During the private speech condition in the present study, participants were focused on a non-speech task (i.e., the computer task), 
reducing  the  probability  that  they  were  focusing  on  potential  listeners’  perceptions.  The  perceived  absence  of  a  listener  likely 
decreased self-monitoring because there was no concern about how one’s speech would be perceived. Future studies can be designed to 
more specifically test these hypotheses and disentangle the factors that contribute to elimination of stuttering. 

4.3. (A lack of) interference from Social Cognition? 

Alm (2014) proposed that social cognition interferes with a vulnerable speech motor system in people who stutter, and, as a result, 
stuttering events occur. Social cognition may include the social importance placed on the situation and potential consequences of the 
situation. For people who stutter, this may involve the risk for stuttering and uncertainty about how to proceed in light of stuttering 
(Alm, 2014). As described by Alm, two parallel networks underlie goal-directed actions such as speaking. The social-cognitive network 
includes medial prefrontal cortex, temporal parietal junction, and anterior cingulate cortex; these brain regions are associated with the 
processing of meaningful and socially relevant speech, social reasoning, and maintaining eye contact with a communicative partner (e. 
g.,  Alexandrou,  Saarinen,  M¨akel¨a,  Kujala,  &  Salmelin,  2017;  Decety  &  Lamm,  2007;  Hartwigsen,  Neef,  Camilleri,  Margulies,  & 
Eickhoff, 2018; Hirsch, Adam Noah, Zhang, Dravida, & Ono, 2018). These regions also comprise the default mode network, which is 
active during rest and non-goal-directed behavior (Fox et al., 2005; Moran, Macrae, Heatherton, Wyland, & Kelley, 2006; Spreng & 
Andrews-Hanna, 2015). The goal-directed network underlies behavior such as propositional speech and includes, but is not limited to, 
dorsolateral  prefrontal  cortex,  supplementary  motor  area,  and  inferior  frontal  gyrus  (Fox  et  al.,  2005;  Spreng,  Sepulcre,  Turner, 
Stevens, & Schacter, 2013). These two networks work in concert during goal-directed behavior such that when one is active the other is 
not. An active social-cognitive or default mode network during speech may cause a speaker’s system to cross a threshold for disruption, 
triggering stuttering events (Alm, 2014). During private speech, there would be no interference from the social-cognitive network 
because there is no need to process information about a listener, no social evaluation, and no communicative intent. Therefore, our 
results provide some support for Alm’s (2014) proposal that stuttering results from interference from social cognition. 

4.4. Speech motor factors 

Despite our attempts to create two conditions (i.e., private speech, private speech+) in which the only manipulated variable was 
the perceived presence of a listener, there may have been variables that changed between the two conditions, particularly in the speech 
motor domain. Speakers who stutter may exhibit slower overall speech rates than speakers who do not stutter during perceptually 
fluent speech (Ingham, Grafton, Bothe, & Ingham, 2012; Kell et al., 2009), due in part to the time that is taken by stuttering events. 
Further, reducing speech rate may lead to increased fluency (Adams, Lewis, & Besozzi, 1973). Therefore, we compared speech rate 
between the private speech and private speech+ conditions. Interestingly, speech rate did not differ between conditions, indicating 
that the virtual elimination of stuttering was not due to a reduction in speech rate. 

It is also possible that prosody contributes to the manifestation of stuttering events (see Packman, 2012; Perkins, Kent, & Curlee, 
1991; Wingate, 1984). In the current study, we found that variations in prosody were lower in the private speech versus the private 
speech+ condition, indicating that reduced prosodic variation may have been in part responsible for the reduction in overt stuttering 
during private speech. During pilot testing, we instructed participants to, “try to speak in the same way”  in the private speech+
condition, which immediately followed the private speech condition, in an attempt to control for prosodic variation. However, we 
observed that participants appeared to enter a mode of “acting” when trying to imitate themselves during the private speech+ con-
dition. As taking on a novel speaking style can also be fluency-enhancing (Bloodstein, 1950; Bloodstein & Bernstein-Ratner, 2008), this 
instruction was discontinued. In previous talk-alone studies, minimal attention was given to possible linguistic and prosodic differ-
ences when comparing alone and non-alone conditions. Although, Hahn (1940) noted that when participants read without a listener 
present, they spoke more slowly, quietly, and rhythmically than they did in other conditions. The role of prosody in stuttering and 
stuttering  events  has  received  relatively  little  attention,  and  we  believe  that  focused  investigations  of  the  impact  of  prosody  on 
stuttering  will  improve  our  understanding  of  the  variables  that  contribute  to  stuttering  events.  Still,  we  argue  that  prosody  and 
prosodic variation reflect pragmatic choices that speakers make when communicating interpersonally, or when there is a perceived 
listener.  Prosody  functions  as  a  communicative  tool,  for  example,  conveying  meaning  and  signaling  the  end  of  one’s  turn 

JournalofFluencyDisorders70(2021)10587810E.S. Jackson et al.                                                                                                                                                                                                     

(Couper-Kuhlen & Selting, 1996). In this way, prosodic variation would not be required if a listener was not perceived by the speaker, 
or at least it would be reduced. Future work should more closely examining linguistic elements such as prosody to determine how they 
contribute to stuttering events, and to explore whether they might impact stuttering even in the absence of listeners. 

4.5. Considerations 

Private speech is defined by the speaker’s intention or state of mind. As such, we cannot be sure that participants intended to speak 
solely to themselves or that they believed that their speech was not perceived, or would not be perceived, by a listener. However, 
several steps were taken to increase the probability that participants engaged in private speech. Participants were deceived to believe 
that the focus of the study was performance on the computer tasks, not any speech-related outcomes, and that they were not being 
recorded. Many participants indicated with their actions that they were unaware of being observed. For instance, when participants 
needed assistance with the study computers they either waited for the experimenters to return to the room or physically left the room 
to look for experimenters. We did not formally poll participants to determine whether they felt as though they were truly alone and that 
their speech was not going to be observed (as in Kalinowski et al., 1999). However, we did informally assess whether participants 
thought that they were being observed during the private speech condition. Most of the participants, but not all, indicated that they did 
not think that they were being observed. 

Another consideration was the experimenters’ abilities to precisely transcribe the participants’ speech in real time. In part, this was 
a result of the configuration of the recording equipment. In order not to raise suspicion of being recorded, a room microphone was used 
rather than a directional or lapel microphone. This limited the fidelity of the recordings, making accurate transcription difficult in 
some cases (especially given that the private speech produced by the participants was, by definition, directed to themselves and not 
necessarily loud enough to be reliably picked up by the microphone). As a result, the private speech and private speech+ conditions 
were only matched post hoc (i.e., by examining only those utterances that matched). However, real-time transcription was warranted in 
this experiment in order to match linguistic content as closely as possible between the private speech and private speech+ conditions, 
and to be efficient in using participants’ time. Accuracy is a challenge to any investigations which necessitate real-time transcription. 
Future investigations may improve upon these limitations by using higher fidelity audio and video equipment, or by having private 
speech and private speech+ conditions on separate days, though the latter may present its own unique challenges. 

It is also possible that due to our relatively small sample size (n = 24), our results did not capture all potential subgroups of 
stutterers. For example, there may be a small subgroup of stutterers who stutter during private speech or who do not experience 
situational variability. In an online, conference paper, Rasskazov and Rasskazova (2006) presented self-report data for 1485 stutterers 
who responded to the question, “Do you stutter when you speak “alone in a room” (when you are speaking aloud or reading aloud, speaking 
to animals or to a pet, when no one is around to hear you? Seventy-eight participants (or 5.25 %) indicated that they stutter as much as 
they  would  in  a  real  conversation,  suggesting  that  such  a  subgroup  may  exist.  However,  it  is  critical  to  note  that  in  addition  to 
self-report data often being unreliable, these results were not peer-reviewed and thus cannot be verified. Perhaps more importantly, 
participants were not instructed to engage in private speech, nor was private speech elicited as it was in the current study. The portion 
of the instructions, “…when no one is around to hear you” does not preclude that a tape recorder was present and participants would be 
heard at a later time. Still, future work could include a larger sample of stutterers and the current paradigm to assess whether adults 
who stutter stutter while engaging in private speech. 

4.6. Future directions 

Examining private speech in children closer to the onset of stuttering (e.g., 2–4 years of age) can potentially reveal the develop-
mental trajectory of social-cognitive processing in stuttering. Greater amounts of private speech in young children compared to adults 
(Kohlberg et al., 1968; Krafft & Berk, 1998; Manning et al., 1994) makes studying young children in this way appealing. Interestingly, 
Razdolskiy (1965) reported that while adults who stutter exhibited less stuttering in an alone versus social condition, preschool age 
children did not exhibit differences between these same conditions. While methods were not sufficiently reported by Razdolsky, it is 
evident that by utilizing private speech, it would be possible to determine whether children exhibit stuttering in the absence of po-
tential listeners (i.e., during private speech), whether real or imagined. If they do, it would imply that children who stutter do not 
experience the same type of differentiation between social and non-social speech compared to adults. If they do not stutter when 
engaging in private speech, it would provide additional evidence that the speaker’s perception of a listener is a key factor in the 
emergence of stuttering events beginning from the onset of stuttering. 

4.7. Conclusion 

The method of eliciting private speech used in this study represents a novel paradigm within stuttering research. In contrast with 
prior talk-alone studies, the current study was particularly focused on removing or significantly reducing the possibility that speakers 
would perceive that their speech would be heard by a listener. Overall, stuttering was not evident during private speech, except in 
extremely rare instances. These findings suggest that the speaker’s perception of being heard by a listener plays a fundamental role in 
the manifestation of stuttering events, whether this is due to thinking about a listener, the potential for social evaluation, commu-
nicative intent, prosodic variation, or something else. Future studies can tease apart these elements to determine what it is about a 
perceived listener that triggers stuttering events. In addition, future studies should use this approach to examine young children who 
stutter, which may answer fundamental questions about the role of speaker perceptions of a listener in the etiology of stuttering. 

JournalofFluencyDisorders70(2021)10587811E.S. Jackson et al.                                                                                                                                                                                                     

Declaration of Competing Interest 

None. 

Acknowledgements 

We thank all of the adults who participated in this study. Without adult stutterers’ willingness to participant, stuttering research 
would not be possible. We also thank Jake Enslin, Hannah Capasci, Hodeena Elsayeed, Yaehee Sung, and Beth Strumpf for their 
contributions  to  the  study.  This  work  was  supported  by  grant  R21DC017821  from  the  National  Institute  on  Deafness  and  Other 
Communication Disorders (awarded to Eric S. Jackson). 

References 

Adams, M. R., Lewis, J. I., & Besozzi, T. E. (1973). The effect of reduced reading rate on stuttering frequency. Journal of Speech and Hearing Research, 16(4), 671–675. 
Alderson-Day, B., & Fernyhough, C. (2015). Inner speech: Development, cognitive functions, phenomenology, and neurobiology. Psychological Bulletin, 141(5), 931. 
Alexandrou, A. M., Saarinen, T., M¨akel¨a, S., Kujala, J., & Salmelin, R. (2017). The right hemisphere is highlighted in connected natural speech production and 

perception. NeuroImage, 152, 628–638. 

Alm, P. A. (2014). Stuttering in relation to anxiety, temperament, and personality: Review and analysis with focus on causality. Journal of Fluency Disorders, 40, 5–21. 
Andrews, G., Howie, P. M., Dozsa, M., & Guitar, B. E. (1982). Stuttering: Speech pattern characteristics under fluency-inducing conditions. Journal of Speech Language 

and Hearing Research, 25(2), 208–216. 

Arends, N., Povel, D.-J., & Kolk, H. (1988). Stuttering as an attentional phenomenon. Journal of Fluency Disorders, 13(2), 141–151. 
Bates, D., M¨achler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. 
Bauerly, K. R., Jones, R. M., & Miller, C. (2019). Effects of social stress on autonomic, behavioral, and acoustic parameters in adults who stutter. Journal of Speech 

Language and Hearing Research, 62(7), 2185–2202. 

Bergmann, G. (1987). Stuttering as a prosodie disturbance: A link between speech execution and emotional processes. Speech motor dynamics in stuttering (pp. 

393–407). Springer. 

Berk, L. E. (1999). Children’s private speech: An overview of theory and the status of research. Lev Vygotsky: Critical Assessments: Thought and Language, 2, 33–70. 
Bloodstein, O. (1950). A rating scale study of conditions under which stuttering is reduced or absent. The Journal of Speech and Hearing Disorders, 15(1), 29–36. 
Bloodstein, O., & Bernstein-Ratner, N. (2008). A handbook on stuttering (6th ed.). Thomson-Delmar.  
Bluemel, C. S. (1935). Stammering and allied disorders. 
Boersma, P., & Weenink, D. (2020). Praat: Doing phonetics by computer (6.1.36) [Computer software]. http://www.praat.org/. 
Bosshardt, H.-G. (1999). Effects of concurrent mental calculation on stuttering, inhalation and speech timing. Journal of Fluency Disorders, 24(1), 43–72. 
Brin, M. F., Stewart, C., Blitzer, A., & Diamond, B. (1994). Laryngeal botulinum toxin injections for disabling stuttering in adults. Neurology, 44(12), 2262–2262. 
Buhr, A. P., Scofield, J., Eyer, J. C., & Walden, T. A. (2017). Cued self-awareness and speech fluency. Speech Language and Hearing, 20(4), 187–195. 
Byrd, C., Coalson, G., & Bush, C. (2010). The communicative intent of stuttered utterances. Journal of Interactional Research in Communication Disorders, 1(2), 253–275. 
Commodore, R. W., & Cooper, E. B. (1978). Communicative stress and stuttering frequency during normal, whispered, and articulation-without-phonation speech 

modes. Journal of Fluency Disorders, 3(1), 1–12. 

Couper-Kuhlen, E., & Selting, M. (1996). Towards an interactional perspective on prosody and a prosodic perspective on interaction. Prosody in conversation: 

Interactional studies. Cambridge University Press.  

Decety, J., & Lamm, C. (2007). The role of the right temporoparietal junction in social interaction: How low-level computational processes contribute to meta- 

cognition. The Neuroscientist : A Review Journal Bringing Neurobiology, Neurology and Psychiatry, 13(6), 580–593. 

Duncan, R. M., & Cheyne, J. A. (2001). Private speech in young adults: Task difficulty, self-regulation, and psychological predication. Cognitive Development, 16(4), 

889–906. 

Eichorn, N., Marton, K., Schwartz, R. G., Melara, R. D., & Pirutinsky, S. (2016). Does working memory enhance or interfere with speech fluency in adults who do and 

do not stutter? Evidence from a dual-task paradigm. Journal of Speech Language and Hearing Research, 59(3), 415–429. 

Eisenson, J., & Horowitz, E. (1945). The influence of propositionality on stuttering. The Journal of Speech Disorders, 10(3), 193–197. 
Fox, M. D., Snyder, A. Z., Vincent, J. L., Corbetta, M., Van Essen, D. C., & Raichle, M. E. (2005). The human brain is intrinsically organized into dynamic, 

anticorrelated functional networks. Proceedings of the National Academy of Sciences, 102(27), 9673–9678. 

Hahn, E. F. (1940). A study of the relationship between the social complexity of the oral reading situation and the severity of stuttering. The Journal of Speech 

Disorders, 5(1), 5–14. 

Hartwigsen, G., Neef, N. E., Camilleri, J. A., Margulies, D. S., & Eickhoff, S. B. (2018). Functional segregation of the right inferior frontal gyrus: Evidence from 

coactivation-based parcellation. Cerebral Cortex, 29(4), 1532–1546. 

Hirsch, J., Adam Noah, J., Zhang, X., Dravida, S., & Ono, Y. (2018). A cross-brain neural mechanism for human-to-human verbal communication. Social Cognitive and 

Affective Neuroscience, 13(9), 907–920. 

Hood, S. B. (1974). Effect of communicative stress on the frequency and form-types of disfluent behavior in adult stutterers. Journal of Fluency Disorders, 1(3), 36–47. 
Ingham, R. J., Bothe, A. K., Jang, E., Yates, L., Cotton, J., & Seybold, I. (2009). Measurement of speech effort during fluency-inducing conditions in adults who do and do not 

stutter. 

Ingham, R. J., Grafton, S. T., Bothe, A. K., & Ingham, J. C. (2012). Brain activity in adults who stutter: Similarities across speaking tasks and correlations with 

stuttering frequency and speaking rate. Brain and Language, 122(1), 11–24. 

Jackson, E. S., Tiede, M., Beal, D., & Whalen, D. H. (2016). The impact of social–cognitive stress on speech variability, determinism, and stability in adults who do and 

do not stutter. Journal of Speech Language and Hearing Research, 59(6), 1295–1314. 

Johnson, W., & Rosen, L. (1937). Studies in the psychology of stuttering: VIIEffect of certain changes in speech pattern upon frequency of stuttering. The Journal of 

Speech Disorders, 2(2), 105–110. 

Kal, E. C., Van der Kamp, J., & Houdijk, H. (2013). External attentional focus enhances movement automatization: A comprehensive test of the constrained action 

hypothesis. Human Movement Science, 32(4), 527–539. 

Kalinowski, J., Stuart, A., Wamsley, L., & Rastatter, M. P. (1999). Effects of monitoring condition and frequency-altered feedback on stuttering frequency. Journal of 

Speech Language and Hearing Research, 42(6), 1347–1354. 

Kell, C. A., Neumann, K., von Kriegstein, K., Posenenske, C., von Gudenberg, A. W., Euler, H., et al. (2009). How the brain repairs stuttering. Brain, 132(10), 

2747–2760. 

Kohlberg, L., Yaeger, J., & Hjertholm, E. (1968). Private speech: Four studies and a review of theories. Child Development, 691–736. 
Krafft, K. C., & Berk, L. E. (1998). Private speech in two preschools: Significance of open-ended activities and make-believe play for verbal self-regulation. Early 

Childhood Research Quarterly, 13(4), 637–658. 

Kuznetsova, A., Brockhoff, P. B., & Christensen, R. H. B. (2017). lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software, 82(1), 1–26. 
Langov´a, J., & 
MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk (3rd edition). 

ˇ
Sv´ab, L. (1973). Reduction of stuttering under experimental social isolation. Folia Phoniatrica et Logopaedica, 25(1-2), 17–22. 

JournalofFluencyDisorders70(2021)10587812E.S. Jackson et al.                                                                                                                                                                                                     

Manning, B. H., White, C. S., & Daugherty, M. (1994). Young children’s private speech as a precursor to metacognitive strategy use during task engagement. Discourse 

Processes, 17(2), 191–211. 

Martin, R. R., & Haroldson, S. K. (1988). An experimental increase in stuttering frequency. Journal of Speech Language and Hearing Research, 31(2), 272–274. 
McNevin, N. H., Shea, C. H., & Wulf, G. (2003). Increasing the distance of an external focus of attention enhances learning. Psychological Research, 67(1), 22–29. 
Moran, J. M., Macrae, C. N., Heatherton, T. F., Wyland, C. L., & Kelley, W. M. (2006). Neuroanatomical evidence for distinct cognitive and affective components of 

self. Journal of Cognitive Neuroscience, 18(9), 1586–1594. 

Oomen, C. C., & Postma, A. (2002). Limitations in processing resources and speech monitoring. Language and Cognitive Processes, 17(2), 163–184. 
Packman, A. (2012). Theory and therapy in stuttering: A complex relationship. Journal of Fluency Disorders, 37(4), 225–233. 
Perkins, W. H., Kent, R. D., & Curlee, R. F. (1991). A theory of neuropsycholinguistic function in stuttering. Journal of Speech Language and Hearing Research, 34(4), 

734. 

Perkins, W. H., Rudas, J., Johnson, L., & Bell, J. (1976). Stuttering: Discoordination of phonation with articulation and respiration. Journal of Speech and Hearing 

Research, 19(3), 509–522. 

Piaget, J. (2002). The language and thought of the child (Vol. 5). Psychology Press.  
Porter, H., & von, K. (1939). Studies in the psychology of stuttering, XIV: Stuttering phenomena in relation to size and personnel of audience. The Journal of Speech and 

Hearing Disorders, 4(4), 323. 

Prizant, B. M., & Wetherby, A. M. (1987). Communicative intent: A framework for understanding social-communicative behavior in autism. Journal of the American 

Academy of Child and Adolescent Psychiatry, 26(4), 472–479. 

Quinn, P. T. (1971). Stuttering: Some observations on speaking when alone. Journal of the Australian College of Speech Therapists, 21(2), 92–94. 
R Core Team. (2020). R: A language and environment for statistical computing. Vienna, Austria: 774 R Foundation for Statistical Computing. Available at: http://www.R- 

project.org/. 

Ramig, P. R., Krieger, S. M., & Adams, M. R. (1982). Vocal changes in stutterers and nonstutterers when speaking to children. Journal of Fluency Disorders, 7(3), 

369–384. 

Rasskazov, I., & Rasskazova, N. (2006). Why do so many stutterers fail to stutter when alone and how can this phenomonen be used in treatment? International 

Stuttering Awareness Day Online Conference. 

Razdolskiy, V. A. (1965). On the speech of stutterers when alone. Zh. Nevropat. Psikhiat (In Russian), 65, 1717–1720. 
Riley, G. D. (2009). SSI-4 stuttering severity instrument. Pro-Ed, Inc.  
Sayles, D. G. (1971). Cortical excitability, perseveration, and stuttering. Journal of Speech and Hearing Research, 14(3), 462–475. 
Sheehan, J., Hadley, R., & Gould, E. (1967). Impact of authority on stuttering. Journal of Abnormal Psychology, 72(3), 290. 
Soskin, W. F., & John, V. P. (1963). The study of spontaneous talk. The stream of behavior (pp. 228–281). New York: Appleton-Century-Crofts. 
Spreng, R. N., & Andrews-Hanna, J. R. (2015). The default network and social cognition. Brain Mapping: An Encyclopedic Reference, 1316, 165–169. 
Spreng, R. N., Sepulcre, J., Turner, G. R., Stevens, W. D., & Schacter, D. L. (2013). Intrinsic architecture underlying the relations among the default, dorsal attention, 

and frontoparietal control networks of the human brain. Journal of Cognitive Neuroscience, 25(1), 74–86. 

Steer, M. D., & Johnson, W. (1936). An objective study of the relationship between psychological factors and the severity of stuttering. Journal of Abnormal and Social 

Psychology, 31(1), 36. 

ˇ
Sv´ab, L., Gross, J., & L´angov´a, J. (1972a). Stuttering and social isolation. The Journal of Nervous and Mental Disease, 155, 1–5. 
ˇ
Sv´ab, L., Gross, J., & L´angov´a, J. (1972b). Stuttering and social isolation. The Journal of Nervous and Mental Disease, 155, 1–5. 
Vasic, N., & Wijnen, F. (2005). 13 stuttering as a monitoring deficit. Phonological encoding and monitoring in normal and pathological speech, 226. 
Vygotsky, L. S. (1934). Thinking and speech. The collected works of LS Vygotsky (Vol. 1). Problems of general psychology. N. Minick (Trans.). New York. 
Wingate, M. E. (1984). Stuttering as a prosodic disorder. Nature and Treatment of Stuttering: New Directions, 215–235. 
Winsler, A., Fernyhough, C., & Montero, I. (2009). Private speech, executive functioning, and the development of verbal self-regulation. Cambridge: Cambridge University 

Press.  

Yairi, E., & Ambrose, N. (1992). A longitudinal study of stuttering in children. Journal of Speech Language and Hearing Research, 35(4), 755–760. 
Yairi, E., & Ambrose, N. G. (2005). Early childhood stuttering for clinicians by clinicians. Pro Ed.  
Yaruss, J. S., & Quesal, R. W. (2016). OASES: Overall assessment of the speaker’s experience of stuttering. Pearson.  
Yovetich, W. S., & Dolgoy, S. (2001). The impact of listeners’ facial expressions on the perceptions of speakers who stutter. Journal of Speech Language Pathology and 

Audiology, 25(3), 145–151. 

Eric S. Jackson, PhD, CCC-SLP is Assistant Professor of Communicative Sciences and Disorders and Director of the stuttering and vvariability (savvy) lab at New York 
University. He studies the contextual variability of stuttering and the factors that drive it, including anticipation and social-cognitive processing. 

Lindsay R. Miller is a Speech-Language Pathologist Clinical Fellow working in New York City schools. She holds a master’s degree in communicative sciences and 
disorders from New York University, where she developed both clinical and research interests in stuttering. 

Haley J. Warner is a Speech-Language Pathologist Clinical Fellow and Research Assistant in the stuttering and vvariability (savvy) lab at New York University. She 
earned her master’s degree in Communicative Sciences and Disorders from New York University. 

J. Scott  Yaruss, PhD,  CCC-SLP, BCS-F, F-ASHA  is a  Professor of Communicative Sciences and  Disorders at Michigan  State University.  His research examines the 
experience of stuttering from the perspective of those who stutter, as well as the variability of stuttering across situations and over time. 

JournalofFluencyDisorders70(2021)10587813
```